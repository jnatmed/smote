{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentacion con tres clases de variantes del metodo SMOTE. \n",
    "\n",
    "Resumen. El objetivo en este documento, es la realizacion de practicas de diferentes implementaciones de SMOTE. Mas precisamente se centra en tres tipos de filtrado sobre las instancias, una vez aplicado el sobremuestreo a las clases minoritarias. Estas metodos son: *SMOTE + Tomek*, *SMOTE + IPF* y *SMOTE + ENN*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Primer variante, filtrado de instancias superpuestas y ruidosas: metodo de SMOTE + TomeK Links: \n",
    "\n",
    "Filtrado de instancias generadas con ruido: Las primeras extensiones de SMOTE motivadas por su conocido inconveniente de generar ejemplos superpuestos y ruidosos fue la adición de un paso de filtrado de ruido justo después de que finaliza el proceso de SMOTE. Dos técnicas típicas son SMOTE-TomekLinks y SMOTE+ENN (Batista et al., 2004). El filtrado de ejemplos artificiales es una operación frecuente que respalda el éxito de SMOTE en datos reales.\n",
    "\n",
    "#### Enlaces de Tomek \n",
    "enlaces de Tomek [22] se pueden definir de la siguiente manera: dados dos ejemplos, $E_i$ y $E_j$ pertenecen a diferentes clases, y d($E_i$, $E_j$) es la distancia entre $E_i$ y $E_j$. Un par ($E_i$, $E_j$) se denomina enlace Tomek si no hay un ejemplo $E_l$, tal que d($E_i$, $E_l$) < d($E_i$, $E_j$) o d($E_j$, $E_l$) < d($E_i$, $E_j$). Si dos ejemplos forman un enlace Tomek, entonces uno de estos ejemplos es ruido o ambos ejemplos están en el límite. Los enlaces de Tomek se pueden utilizar como método de submuestreo o como método de limpieza de datos. Como método de submuestreo, solo se eliminan los ejemplos pertenecientes a la clase mayoritaria, y como método de limpieza de datos, se eliminan los ejemplos de ambas clases.\n",
    "#### Enlaces Smote + Tomek \n",
    "Aunque el sobremuestreo de ejemplos de clases minoritarias puede equilibrar las distribuciones de clases, no se resuelven algunos otros problemas que normalmente se presentan en conjuntos de datos con distribuciones de clases asimétricas. Con frecuencia, los grupos de clases no están bien definidos, ya que algunos ejemplos de clases mayoritarias podrían estar invadiendo el espacio de clases minoritarias. Lo contrario también puede ser cierto, ya que la interpolación de ejemplos de clases minoritarias puede expandir los grupos de clases minoritarias, introduciendo ejemplos artificiales de clases minoritarias demasiado profundamente en el espacio de la clase mayoritaria. Inducir un clasificador en tal situación puede conducir a un sobreajuste. Para crear grupos de clases mejor definidos, proponemos aplicar enlaces de Tomek al conjunto de entrenamiento sobremuestreado como método de limpieza de datos. Por lo tanto, en lugar de eliminar solo los ejemplos de la clase mayoritaria que forman los enlaces de Tomek, se eliminan los ejemplos de ambas clases. La aplicación de este método se ilustra en la Figura 2. Primero, el conjunto de datos original (a) se sobremuestrea con Smote (b), y luego los enlaces de Tomek se identifican (c) y se eliminan, produciendo un conjunto de datos equilibrado con agrupaciones de clases definidas (d). El método de enlaces Smote + Tomek se utilizó por primera vez para mejorar la clasificación de ejemplos para el problema de anotación de proteínas en Bioinformática [1].\n",
    "\n",
    "Figura 2: Balanceo de un conjunto de datos: conjunto de datos original (a); conjunto de datos sobremuestreados (b); Identificación de enlaces Tomek (c); y eliminación de ejemplos de límites y ruido (d).\n",
    "![Figura 2](papers-smote\\practica-smote\\figura2_smote_tomeklinks.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TomeKlinks \n",
    "# el objetivo es clarificar la distancia entre clases, haciendo la region minoritaria mas distintiva.\n",
    "from smote_variants import SMOTE_TomekLinks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Segunda variante, filtrado basado filtro de ruido iterativo: metodo de SMOTE + IPF: \n",
    "\n",
    "Los conjuntos de datos de clasificación a menudo tienen una distribución de clases desigual entre sus ejemplos. Este problema se conoce como clasificación desequilibrada. La técnica de sobremuestreo de minorías sintéticas (SMOTE) es uno de los métodos de preprocesamiento de datos más conocidos para hacerle frente y equilibrar los diferentes números de ejemplos de cada clase. Sin embargo, como afirman trabajos recientes, el desequilibrio de clases no es un problema en sí mismo y la degradación del rendimiento también está asociada con otros factores relacionados con la distribución de los datos. Uno de ellos es la presencia de ruido y ejemplos límite, estos últimos ubicados en las áreas que rodean los límites de clase. Ciertas limitaciones intrínsecas de SMOTE pueden agravar el problema producido por este tipo de ejemplos.\n",
    "Esta variante de SMOTE propone un filtro de ruido iterativo basado en conjuntos llamado Filtro de particion iterativa (IPF), que puede superar los problemas producidos por ejemplos ruidosos y limite en conjuntos de datos desequilibrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smote_variants import SMOTE_IPF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tercera variante, filtrado mediante el submuestreo metodo de Smote + ENN\n",
    "La motivación detrás de este método es similar a los enlaces Smote + Tomek. ENN tiende a eliminar más ejemplos que los enlaces de Tomek, por lo que se espera que proporcione una limpieza de datos más profunda. A diferencia de NCL (Regla de Limpieza de Vecindarios), que es un método de submuestreo, ENN se usa para eliminar ejemplos de ambas clases. Por lo tanto, cualquier ejemplo mal clasificado por sus tres vecinos más cercanos se elimina del conjunto de entrenamiento.\n",
    "\n",
    "#### 3.1 Regla de limpieza de vecindarios \n",
    "La regla de limpieza de vecindarios (NCL) [15] utiliza  para eliminar ejemplos de clases mayoritarias. La regla de vecinos más cercanos editada (ENN) elimina cualquier ejemplo cuya etiqueta de clase difiera de la clase de al menos dos de sus tres vecinos más cercanos. NCL modifica el ENN para aumentar la limpieza de datos. Para un problema de dos clases, el algoritmo se puede describir de la siguiente manera: para cada ejemplo $E_i$ en el conjunto de entrenamiento, se encuentran sus tres vecinos más cercanos. Si $E_i$ pertenece a la clase mayoritaria y la clasificación dada por sus tres vecinos más cercanos contradice la clase original de $E_i$, entonces se elimina $E_i$. Si $E_i$ pertenece a la clase minoritaria y sus tres vecinos más cercanos clasifican erróneamente a $E_i$, entonces se eliminan los vecinos más cercanos que pertenecen a la clase mayoritaria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smote_variants import SMOTE_ENN"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
