# Un método de conjunto de decisiones de tres vías para el sobremuestreo de datos desequilibrados ✩

La técnica de sobremuestreo de minorías sintéticas (SMOTE) es un método eficaz para la clasificación de datos desequilibrados. Se han propuesto muchas variantes de SMOTE en la última década. Estos métodos se centraron principalmente en cómo seleccionar las muestras minoritarias cruciales que asumen implícitamente que la selección de muestras minoritarias clave es binaria. Por lo tanto, rara vez se considera el costo de la selección de la muestra clave. Con este fin, este documento propone un modelo de decisión de tres vías (CTD) considerando las diferencias en el costo de seleccionar muestras clave. CTD primero usa el algoritmo de cobertura constructiva (CCA) para dividir las muestras minoritarias en varias coberturas. Luego, se construye un modelo de decisión de tres vías para la selección de muestras clave de acuerdo con la densidad de cobertura en las muestras minoritarias. Finalmente, los umbrales α y β correspondientes de CTD se obtienen en función del patrón de distribución de cobertura en muestras minoritarias, después de eso, las muestras clave pueden seleccionarse para el sobremuestreo SMOTE. Además, para superar la escasez de CCA que puede contener elementos no óptimos mediante la selección aleatoria del centro de cobertura, se propone además un modelo de conjunto basado en CTD (CTDE) para mejorar el rendimiento de CTD. Los experimentos numéricos en 10 conjuntos de datos desequilibrados muestran que nuestro método es superior a los métodos de comparación. Al construir el conjunto de la selección de muestra clave basada en decisiones de tres vías, el rendimiento del modelo se puede mejorar de manera efectiva en comparación con varios métodos de vanguardia.

## 1. Introducción 
Los datos desequilibrados existen en muchas aplicaciones prácticas, como la clasificación de texto [1], la detección de tarjetas de crédito engañosas [2], distinguir teléfonos de acoso malicioso [3] y diagnósticos médicos [4–6], etc. La característica del conjunto de datos desequilibrados es que el número de instancias de una clase es significativamente menor que otra [7]. Por conveniencia, la primera se conoce como clase minoritaria (o positiva), y la segunda se denomina clase mayoritaria (o negativa). Los métodos de clasificación tradicionales tienden a mejorar la tasa de reconocimiento de todas las muestras, y la tasa de reconocimiento de las muestras de la clase minoritaria es fácil de ignorar. Sin embargo, en muchos casos, es más importante identificar las muestras de clase positiva que identificar las muestras de clase negativa. Por ejemplo, en el diagnóstico médico, si una persona sana es mal diagnosticada como paciente, traerá una carga mental para el paciente. Sin embargo, si un paciente es mal diagnosticado como una persona sana, puede perder el mejor período de tratamiento y puede tener consecuencias muy graves. El costo de diagnosticar erróneamente a un paciente como persona sana es mucho mayor que el costo de diagnosticar erróneamente a una persona sana como paciente. Por lo tanto, cómo mejorar de manera efectiva la precisión de la clasificación en muestras minoritarias y la precisión general en los conjuntos de datos desequilibrados simultáneamente se ha convertido en un tema de investigación en el campo del aprendizaje automático. Actualmente, las investigaciones sobre la clasificación de datos desequilibrados se pueden dividir aproximadamente en dos aspectos: nivel de algoritmo y nivel de conjunto de datos. Para el primero, diseñar un algoritmo de clasificación más eficiente es el enfoque principal. Y para este último, cómo generar un conjunto de datos equilibrado a partir del conjunto de datos desequilibrado original es el punto crítico de la investigación. No hay consenso sobre cuál de los dos enfoques diferentes es mejor. Sin embargo, hay más estudios a nivel de conjunto de datos que a nivel de algoritmo. En otras palabras, las investigaciones están más preocupadas por el nivel del conjunto de datos que incluye el sobremuestreo, el submuestreo y el modelo híbrido de ambos. La solución a nivel de conjunto de datos es equilibrar la distribución de la clase minoritaria y mayoritaria [8]. La técnica de sobremuestreo de minorías sintéticas (SMOTE) [3] es una de las técnicas de sobremuestreo más representativas. SMOTE puede resolver eficazmente el problema de sobreajuste del sobremuestreo aleatorio y mejorar la capacidad de aprendizaje del clasificador descendente que se entrenó con los datos equilibrados generados por SMOTE. Sin embargo, SMOTE genera el mismo número de muestras sintéticas para cada una de las muestras minoritarias que ignora la distribución de los vecinos más cercanos. SMOTE también podría generar muestras superpuestas y muestras disjuntas, lo que aumenta la dificultad de la clasificación aguas abajo. Para superar la deficiencia de SMOTE, algunos académicos han presentado muchos algoritmos mejorados de SMOTE en los últimos años. El algoritmo Borderline-SMOTE fue propuesto por Han et al. [9], que usa KNN para encontrar muestras minoritarias cerca del límite para generar las muestras sintéticas. Supera la ceguera de SMOTE en la selección de muestras minoritarias. Mientras tanto, hasta cierto punto, puede mejorar la tasa de reconocimiento de muestras minoritarias. Sin embargo, el algoritmo todavía no consideró la distribución espacial de los datos. Él et al. [10] propusieron ADASYN, un algoritmo de muestreo sintético adaptativo. Este algoritmo utiliza principalmente la distribución de densidad como estándar para determinar automáticamente el número de muestras sintéticas y cambia los pesos de diferentes muestras minoritarias de forma adaptativa para generar muestras sintéticas para cada una de las muestras minoritarias. Barúa et al. [11] propusieron una técnica de sobremuestreo de minoría ponderada por mayoría (MWMOTE). El método primero identifica las muestras informativas de clases minoritarias difíciles de aprender y les asigna pesos de acuerdo con su distancia euclidiana de las muestras de clases mayoritarias más cercanas. A continuación, genera las muestras sintéticas a partir de las muestras de clases minoritarias informativas ponderadas utilizando un enfoque de agrupación. Finalmente, esto se hace de tal forma que todas las muestras generadas queden dentro de unos clusters con clase minoritaria. Este método afirma que puede evitar el efecto de las muestras de ruido sintético. Sin embargo, la forma de detectar de manera efectiva la muestra difícil de aprender es difícil, por lo que este método aún no puede seleccionar muestras informativas de manera adecuada.

Batista et al. [12] propusieron un método de mejora (SMOTE+TomekLink) para SMOTE desde otro punto de vista que es diferente a los métodos anteriores. Este método utiliza TomekLink como método de limpieza de datos, que se aplica al conjunto de datos balanceado obtenido por SMOTE. La estrategia en TomekLink es eliminar las muestras vecinas más cercanas en pares. Hasta cierto punto, las muestras de error introducidas por SMOTE se pueden eliminar para mejorar el rendimiento del modelo de clasificación posterior. Sin embargo, este método puede eliminar muestras minoritarias representativas cerca del límite de clasificación del conjunto de datos original. Además, es difícil determinar el estándar de juicio de distancia para muestras heterogéneas, lo que afecta la estabilidad del algoritmo. Los algoritmos anteriores han mejorado SMOTE desde diferentes puntos de vista. Pero todos son binarios en la selección de muestras clave. La discusión sobre la relación entre la selección de la muestra clave y el desempeño de la clasificación final es rara. Cuando algunas de las muestras minoritarias están relativamente concentradas, SMOTE no puede mejorar de manera efectiva el rendimiento de la clasificación mediante el sobremuestreo de estas muestras. Por el contrario, aumentará la complejidad del algoritmo. En otras palabras, la consideración del costo de la selección de muestras clave para mejorar el desempeño de la clasificación es insuficiente. ***Para resolver los problemas anteriores, este documento propone un método de muestreo de decisión de tres vías (CTD) basado en el algoritmo de cobertura constructiva (CCA) [13,14]. CTD primero usa CCA para construir la cobertura de los datos desequilibrados. Luego, las cubiertas de las muestras minoritarias se seleccionan y dividen en tres regiones según la densidad de la cubierta. Finalmente, los umbrales correspondientes α y β se obtienen en función de los patrones de distribución de cobertura y luego se pueden seleccionar muestras clave para el sobremuestreo SMOTE.*** Teniendo en cuenta las incertidumbres causadas por CCA que eligen aleatoriamente el centro de cobertura, ***este artículo propone además un modelo de conjunto (CTDE) basado en CTD para mejorar la eficiencia del algoritmo***. El resto del documento está organizado de la siguiente manera. La Sección 2 revisa brevemente algunos trabajos relacionados. La sección 3 presenta nuestro enfoque. Se realizaron experimentos numéricos para validar la efectividad de CTDE en la sección 4. Y la sección 5 concluye el documento. 

2. Preliminar 

2.1. SMOTE 

SMOTE [3] es un método de sobremuestreo clásico, que utiliza la interpolación lineal para sintetizar nuevas muestras para la clase minoritaria. En primer lugar, se utiliza la distancia euclidiana para encontrar el k-vecino más cercano para cada una de las muestras minoritarias originales. Luego, SMOTE usa la interpolación lineal [15] para sintetizar muestras minoritarias entre cada muestra minoritaria y sus k vecinos más cercanos de clase minoritaria. Finalmente, las muestras sintéticas se agregaron al conjunto de datos original. SMOTE puede reducir el grado de desequilibrio aumentando el número de muestras minoritarias del conjunto de datos. El procedimiento de SMOTE se muestra en la Tabla 1.

2.2. Algoritmo de cobertura constructiva 

El algoritmo de cobertura constructiva (CCA) [13] puede considerarse como una red neuronal de tres capas que se realiza cubriendo muestras con la misma etiqueta de forma constructiva. Sea X = {(x1, y1),(x2, y2),...,(xp, yp)} un conjunto de datos dado, donde xi = (x1 i , x2 i ,..., xn i ) y yi representan el atributo de decisión de la i-ésima muestra.

Capa de entrada: un total de n neuronas. Cada neurona corresponde a una dimensión de la muestra. En otras palabras, el atributo característico de la muestra es xi = (x1 i , x2 i ,..., xn i ). Las neuronas de esta capa solo se encargan de recibir información externa. Capa oculta: un total de s neuronas. El número de neuronas en la capa oculta es 0 al principio, y luego aumenta monótonamente con el aumento de las cubiertas construidas por CCA hasta cubrir todas las muestras. Después de eso, se puede obtener un conjunto de cobertura C = {C1 1,..., Cn1 1 , C1 2,..., Cn2 2 ,..., C1 m,..., Cnm m }. Donde C j i representa la j-ésima cubierta de la i-ésima clase, que representa una neurona en la capa oculta. Capa de salida: un total de m neuronas. La entrada de la neurona t-ésima es un conjunto de cubiertas con la misma etiqueta de clase, y la salida de la neurona t-ésima es la etiqueta de clase correspondiente Ot = (o1 = 1,..., ot = t,... , om = m). CCA es un tipo de aprendizaje supervisado, que se puede describir de la siguiente manera [13]: 
Paso 1. Normalización. Normalice los datos a [0, 1]. 
Paso 2. Proyecte las muestras a un espacio de superficie esférica Sn+1 con n + 1 dimensión.
Paso 3. Construye neuronas ocultas. Se selecciona aleatoriamente una muestra xk en X como centro de cobertura, es decir, uno de los pesos w de las neuronas de la capa oculta. CCA utiliza el producto interno en lugar de la distancia euclidiana para calcular la distancia entre muestras. El umbral θ (radio de la cubierta) correspondiente a w se puede calcular mediante los siguientes pasos: 
Producto interior: 

xk, xi = x1 k x1 i +···+ x n+1 kx n+1 i , i ∈ {1, 2,..., p} (2.2) 

Calcular el producto interior máximo (equivalente a la distancia mínima): d1(k) = max yi =yk { xk, xi }, i ∈ {1, 2 ,..., p} (2.3) 

Calcular el producto interior mínimo (equivalente a la distancia máxima):

$d_2(k)$ = $min_{y_1=y_k}$ yi=yk xk, xi | xk, xi > d1(k) , i ∈ {1, 2,..., p} (2.4) Calcula el radio de cobertura:

$$θ=\frac{d_1(k)+d_2(k)}{2}$$ (2.5)

Se puede obtener una neurona (w, θ) de acuerdo con (2.2)–(2.5), que contiene un conjunto de muestras con la misma etiqueta de clase que xk. Y las muestras dentro de la cubierta están marcadas como "aprendidas". Paso 4. Retire las muestras en (w, θ) y repita el paso 3 hasta que se eliminen todas las muestras del espacio Sn+1. Del proceso de CCA, se puede ver que si una muestra está cubierta, entonces no participa en la siguiente ronda del proceso de construcción de la cubierta, el tiempo de cálculo del algoritmo puede reducirse considerablemente.

2.3. Decisión tripartita 

La decisión tripartita es una extensión de la decisión bidireccional tradicional [16–23,27,28]. En la decisión bidireccional, solo se consideran dos opciones, aceptación o rechazo. Sin embargo, en las aplicaciones prácticas, a menudo es imposible aceptarlo o rechazarlo debido a la inexactitud o la incompletitud de la información. En este caso, la decisión de tres vías se usa inconscientemente. La teoría de la decisión de tres vías se propone en el conjunto aproximado [24, 25] y el conjunto aproximado de decisión [26–33] para proporcionar una explicación semántica razonable para los tres dominios del conjunto aproximado. Los dominios positivo, negativo y límite de un modelo de conjunto aproximado pueden interpretarse como el resultado de tres decisiones: aceptación, rechazo y no compromiso. 

Yao propuso un modelo de conjunto aproximado teórico de la decisión (DTRS) mediante la introducción de la minimización del riesgo de la decisión bayesiana en la investigación del conjunto aproximado [16,24,26]. Al calcular los valores de pérdida de riesgo de todo tipo de decisiones de clasificación, DTRS puede averiguar la decisión de riesgo mínimo esperado. Luego se usa para dividir el dominio positivo (POS), el dominio límite (BND) y el dominio negativo (NEG). Muchos académicos han realizado más estudios sobre el método de decisión de tres vías y sus aplicaciones [34–47]. Sin pérdida de generalidad, sea Ω = {S,¬S} el conjunto de estados, donde S y ¬S son complementarios. El conjunto de acciones A = (aP ,aB ,aN ), donde aP , aB y aN denotan las acciones que deciden objeto de dominio positivo (POS(X)), dominio negativo NEG(X) y dominio límite (BND(X)) respectivamente. La matriz de la función de pérdida para acciones en diferentes estados se proporciona en la Tabla 2. En la Tabla 2, por ejemplo, λPP, λBP y λNP representan los valores de la función de pérdida cuando las acciones son aP, aB y aN y el objeto está en el estado S. De manera similar , λPN, λBN y λNN representan los valores de la función de pérdida cuando las acciones son aP , aB y aN y el objeto está en el estado ¬S. De acuerdo con la matriz de función de pérdida y un proceso de derivación, las reglas de decisión DTRS se pueden obtener [16,26,48,49]:

(1) Si P(S|X) ≥ α, entonces decida POS(X); 
(2) Si β < P(S|X) < α, entonces decida BND(X); 
(3) Si P(S|X) ≤ β, entonces decida NEG(X);

### 3. Método propuesto 

#### 3.1. Modelo de decisión de tres vías (CTD) basado en CCA 

Para la clasificación de datos desequilibrados, la tasa de reconocimiento de muestras minoritarias y la tasa de reconocimiento de todas las muestras deben considerarse simultáneamente. Para mejorar la precisión de las muestras minoritarias, el muestreo de muestras clave de clases minoritarias es uno de los métodos para mejorar el rendimiento de la clasificación. Por lo tanto, la forma de seleccionar las muestras clave es muy importante. En la clase minoritaria, diferentes muestras tienen diferentes impactos en el rendimiento del sobremuestreo. Generalmente, las muestras en el límite de la clase minoritaria son más críticas que las de los centros minoritarios. Sin embargo, los métodos de sobremuestreo existentes solo consideran las muestras de la región límite. Esos métodos asumen que la selección de muestras minoritarias clave es binaria y se ignora el efecto de las muestras no límite en el rendimiento del algoritmo. En realidad, al seleccionar muestras minoritarias para el sobremuestreo de datos desequilibrados, las muestras se pueden clasificar en tres categorías: 

(1) Muestras con un efecto de agregación evidente en la clase minoritaria, tienen poco impacto en el rendimiento del modelo. 

(2) Muestras de ruido o muestras aisladas en clase minoritaria, que están lejos de las muestras de clase minoritaria que tienen un efecto de agregación evidente. La selección de esas muestras no solo no puede mejorar el rendimiento del modelo, sino que también tiene efectos negativos en el rendimiento del modelo.

(3) Las muestras minoritarias no pertenecen a las dos categorías anteriores, que juegan un papel vital en la mejora de la capacidad de reconocimiento de la clase minoritaria. Es importante respaldar la delineación del límite de clasificación, y la selección eficiente de estas muestras puede mejorar efectivamente el rendimiento de la clasificación aguas abajo. 

Inspirados en la decisión de tres vías, los tres tipos de muestras minoritarias mencionadas anteriormente podrían relacionarse naturalmente con los tres dominios del modelo de decisión de tres vías. La primera categoría, la segunda categoría y la tercera categoría corresponden al dominio positivo (POS), el dominio negativo (NEG) y el dominio límite (BND) de decisión de tres vías, respectivamente. La Fig. 1 muestra el diagrama esquemático del proceso de construcción de CTD. 

Con la descripción anterior, sabemos que la prioridad de investigación en este estudio es cómo excavar las muestras de la clase minoritaria y luego construir el POS, NEG y BND del modelo de decisión de tres vías. Para un conjunto de datos dado X = {(x1, y1),(x2, y2),...,(xp, yp)} contiene p muestras con n dimensiones y m clases. Sea xi = (x1 i , x2 i ,..., xn i ) represente el atributo de característica n-dimensional de la i-ésima muestra y yi represente el atributo de decisión de la i-ésima muestra, e i = 1, 2,. .., pags. Se puede obtener un juego de tapas por CCA: C = {C1 1, C2 1,..., Cn1 1 , C1 2, C2 2,..., Cn2 2 ,..., C1 m, C2 m,. .., Cnm m }. Sea Ci = ∪C j i , donde j = 1, 2,...,ni. Y Ci representa un conjunto de tapas con la misma etiqueta de clase. Y se pueden generar tres dominios según estas coberturas. Por conveniencia, suponiendo que el conjunto de datos desequilibrado tiene dos categorías (minoritaria y mayoritaria) de muestras, C0 = (C1 0, C2 0,..., Cn0 0) y C1 = (C1 1, C2 1,..., Cn1 1) representan todas las coberturas de la clase minoritaria y la clase mayoritaria respectivamente. Sea Ct 0 la t-ésima cobertura de C0, la densidad de Ct 0 se puede definir de la siguiente manera:


![textp alternativo](https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png)
