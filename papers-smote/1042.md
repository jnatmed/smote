# Desequilibrio múltiple: un software de código abierto para el aprendizaje de desequilibrio de clases múltiples

La clasificación de desequilibrios es uno de los problemas de investigación más desafiantes en el aprendizaje automático. Las técnicas para la clasificación de desequilibrio de dos clases son relativamente maduras en la actualidad, pero el aprendizaje de desequilibrio de múltiples clases sigue siendo un problema abierto. Además, la comunidad carece de una herramienta de software adecuada que pueda integrar los principales trabajos en el campo. En este documento, presentamos Multi-Imbalance, un paquete de software de código abierto para la clasificación de datos desequilibrados de múltiples clases. Proporciona a los usuarios siete categorías diferentes de algoritmos de aprendizaje de desequilibrio multiclase, incluidos los últimos avances en el campo. Los códigos fuente y la documentación de Multi-Imbalance están disponibles públicamente en https://github.com/chongshengzhang/Multi_Imbalance.

1. Introducción 

El aprendizaje de desequilibrio se ha convertido en uno de los principales temas de investigación en el aprendizaje automático. Tiene aplicaciones importantes en la detección de fraudes con tarjetas de crédito [1], diagnóstico de fallas [2], diagnóstico médico [3], reconocimiento de patrones [4], etc. También tiene un gran potencial en seguridad, como la detección de aplicaciones maliciosas [5–7]. ]. Hasta ahora, existen varias herramientas de software para analizar datos desequilibrados de dos clases, a nivel de datos o algoritmos. Sin embargo, para datos desequilibrados multiclase, hay muy pocos paquetes de software disponibles, aunque muchos investigadores han propuesto varios algoritmos y técnicas para abordar este problema [8–10]. En este trabajo, desarrollamos el paquete de software "Multi-Imbalance" (clasificación de datos desequilibrados de clases múltiples) y lo compartimos con la comunidad para impulsar la investigación en este campo. El software Multi-Imbalance desarrollado contiene 18 algoritmos diferentes para el aprendizaje de desequilibrio de múltiples clases, que se muestran en la Fig. 1. Dividimos estos algoritmos en 7 módulos (categorías). Presentaremos el marco y la funcionalidad de este software en las siguientes secciones. Multi-Imbalance permite a los investigadores reutilizar directamente nuestras implementaciones en la clasificación de datos desequilibrados de múltiples clases, evitando así codificarlos desde cero. Por lo tanto, será de gran ayuda para los investigadores e ingenieros en este campo. El resto de este documento está organizado de la siguiente manera: la Sección 2 proporciona los antecedentes sobre la clasificación de desequilibrios, la Sección 3 describe el marco del software, la Sección 4 presenta un ejemplo ilustrativo y la Sección 5 concluye el documento. 2. Antecedentes En esta sección, brindamos una descripción general de las estrategias de descomposición e introducimos los siete módulos en Multi-Desequilibrio. También presentaremos las herramientas de software existentes para el aprendizaje de desequilibrio de dos clases. 

2.1. Clasificación en el campo de aprendizaje de desequilibrio 

Los datos desequilibrados (sesgados) se presentan en muchas aplicaciones del mundo real. Sin embargo, los métodos convencionales de aprendizaje automático no se enfocan en la precisión de la predicción de las clases minoritarias, lo que puede ser más interesante para ciertas aplicaciones. En los últimos años, se han propuesto muchos algoritmos de clasificación de datos desequilibrados de dos clases [11,12], que se pueden dividir en cuatro categorías: soluciones a nivel de datos o a nivel de algoritmo, métodos que son sensibles al costo o basados ​​en conjuntos.

Sin embargo, muchos de los algoritmos anteriores no pueden manejar directamente datos desequilibrados de varias clases, por lo que se ha invertido un esfuerzo significativo en este tema en los últimos años [8–10,13]. Estos algoritmos suelen ser combinaciones de técnicas de binarización que transforman los datos originales de varias clases en subconjuntos binarios, con un algoritmo de clasificación de desequilibrio de dos clases. La figura 2 describe el procedimiento general de estos algoritmos. Los datos desequilibrados de varias clases se dividen primero en dicotomías (equilibradas), luego se entrena un clasificador binario correspondiente en cada dicotomía. Estos clasificadores binarios luego se integran utilizando métodos de aprendizaje de conjuntos, como la votación por mayoría, para hacer predicciones. Las cuatro métricas de precisión comúnmente adoptadas para datos desequilibrados son Acc, AUC, G-Mean y F-Measure. 

2.2. Algoritmos de clasificación de desequilibrios relacionados y técnicas de binarización Como se muestra en la Fig. 2, primero debemos transformar los datos originales de varias clases en subconjuntos binarios a través de estrategias de descomposición. Multi-Imbalance implementa 5 técnicas de binarización diferentes: 

• Enfoque uno contra uno (OVO) [14]: OVO entrena un clasificador binario para cada posible par de clases ignorando los ejemplos que no pertenecen a las clases de pares. 
• One-vs-All (OVA) [15]: OVA entrena un único clasificador para cada clase, considerando la clase actual como minoritaria y el resto de clases como mayoritarias.
• One-Against-Higher-Order (OAHO) [16]: OAHO primero ordena las clases por el número de muestras en orden descendente {C1, C2, . . . , Cn}, donde C1 tiene el mayor número de muestras. Comenzando desde C1 hasta Cn−1, etiqueta secuencialmente la clase actual como "clase positiva" y todas las clases restantes con rangos más bajos como "clases negativas", luego entrena un clasificador binario sobre cada conjunto de datos resultante. 
• All-and-One (A&O) [17]: A&O es una combinación de OVO y OVA. Para una nueva predicción, primero usa OVA para obtener los resultados de predicción top2 (ci, cj), luego adopta el clasificador OVO previamente entrenado para el par de clases que contienen ci y cj para hacer la predicción final. 
• Los códigos de salida de corrección de errores (codificación ECOC) [18]: ECOC utiliza la idea de la codificación de salida de corrección de errores para clasificar los datos multiclase. Primero construye una palabra clave para cada clase para obtener la mayor distancia entre varias clases, transformando así las clases de los datos multiclase en palabras clave c. Multi-Imbalance contiene los siguientes algoritmos de clasificación de datos desequilibrados, pero solo las variantes de AdaBoost admiten la clasificación de datos multiclase.

• FuzzyImb (Clasificación del vecino más cercano promedio ponderado ordenado difuso-áspero desequilibrado, IFROWANN para abreviar):
Propuesto en [19], este algoritmo es un clasificador poderoso para datos desequilibrados de dos clases basado en la teoría de conjuntos aproximados difusos y la agregación promedio ponderada ordenada. 
• imECOC: Propuesto en [20], es un método ECOC [18] adaptado para el aprendizaje de desequilibrio, para cada clasificador binario considera simultáneamente el desequilibrio entre clases y dentro de clases. Decodifica con distancia ponderada para encontrar la palabra clave (clase) más cercana. 
• HDDT (árboles de decisión de distancia de Hellinger): Es una técnica de árboles de decisión que utiliza la distancia de Hellinger como criterio de división [21]. 
• PRMs-IM: Divide aleatoriamente las muestras mayoritarias en m partes (m es la relación entre el número de muestras mayoritarias y minoritarias), luego combina cada parte con todas las instancias minoritarias, luego entrena un clasificador binario correspondiente [22]. 
• Variantes de AdaBoost. AdaBoost (Adaptive Boosting) [23] es originalmente un algoritmo de clasificación binaria que integra múltiples clasificadores débiles para construir un clasificador más fuerte. En Multi-Imbalance, incorporamos cinco algoritmos basados ​​en AdaBoost para manejar datos desequilibrados de varias clases.

– AdaBoost.M1 y SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss function): amplían AdaBoost tanto en la actualización de los pesos de las muestras como en la estrategia de combinación de clasificadores [24]. La principal diferencia entre ellos radica en el método de actualización de los pesos de las muestras. 
– AdaC2.M1: propuesto en [25], este método deriva la mejor configuración de costos a través del algoritmo genético (GA) para el posterior refuerzo. 
– AdaBoost.NC: propuesto en [26], desaprueba GA ya que requiere mucho tiempo, pero enfatiza la diversidad del conjunto durante el entrenamiento. 
– PIBoost: propuesto en [27], utiliza una función de pérdida exponencial basada en márgenes para clasificar datos desequilibrados multiclase.

