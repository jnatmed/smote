# Clasificación de datos desequilibrados utilizando técnicas complementarias de máquina de vectores de soporte difuso y SMOTE

Resumen— Se propone una técnica de muestreo híbrida mediante la combinación de la Máquina de vectores de soporte difusos complementarios (CMTFSVM) y la Técnica de sobremuestreo de minorías sintéticas (SMOTE) para manejar el problema de clasificación desequilibrada. La técnica propuesta utiliza una función de pertenencia optimizada para mejorar el rendimiento de la clasificación y se compara con tres clasificadores diferentes. Los experimentos consistieron en cuatro conjuntos de datos de referencia estándar y un dato del mundo real de células vegetales. Los resultados revelaron que la implementación de CMTFSVM seguida de SMOTE proporcionó mejores resultados que otros clasificadores de FSVM para los conjuntos de datos de referencia. Además, presentó el mejor resultado en un conjunto de datos del mundo real con 0,9589 de G-mean y 0,9598 de AUC. Se puede concluir que las técnicas propuestas funcionan bien con datos de referencia desequilibrados y del mundo real.

## I. INTRODUCCIÓN 
El problema de la distribución desequilibrada en conjuntos de datos del mundo real es común. Ha llamado mucho la atención de los investigadores en las disciplinas de aprendizaje automático, minería de datos y clasificación de patrones [1]. Este problema se refiere a una situación en la que la distribución de clases de los datos es desigual entre las clases de datos. Una clase se considera como clase mayoritaria o clase negativa, cuando supera a la otra clase, clase minoritaria o clase positiva. Muy pocas muestras en la clase minoritaria podrían provocar una detección falsa o que los datos se ignoren como ruido [2]. Estos problemas existen en varios campos, como la recuperación de información, el diagnóstico médico y la clasificación de textos [3]. Una serie de soluciones de nivel de datos propuestas para manejar los problemas de desequilibrio de clases, generalmente submuestreando instancias de clase negativas o sobremuestreando instancias de clase positivas, o usando una combinación de los dos enfoques para lidiar con la distribución de clases sesgada en los datos de entrenamiento. 

Este estudio se centra en la aplicación de la técnica complementaria (CMT), la máquina de vectores de soporte difusos (FSVM) y la técnica de sobremuestreo de minorías sintéticas (SMOTE) para abordar el problema de clasificación desequilibrada. Se implementan tres clasificadores para comparar el rendimiento de clasificación del enfoque propuesto. El rendimiento se evalúa en términos de media geométrica (media G) y área bajo la curva característica operativa del receptor (AUC). 

La estructura del documento es la siguiente. La Sección II revisa la investigación previa sobre el aprendizaje a partir de datos desequilibrados. La Sección III presenta el concepto de la técnica propuesta e información sobre las medidas de evaluación de los clasificadores. Las características de los datos utilizados en los experimentos y el proceso del experimento se presentan en la Sección IV. La Sección V contiene los resultados del rendimiento de clasificación de los conjuntos de datos de referencia y del mundo real. La Sección VI proporciona la conclusión de este estudio.

II. TRABAJO RELACIONADO 
La modificación de la distribución de datos en el conjunto de entrenamiento es una de las soluciones para manejar problemas de clasificación de datos desequilibrados. Los enfoques de submuestreo y sobremuestreo se usan comúnmente para lidiar con la distribución de los conjuntos de datos. Los métodos de submuestreo disminuyen el número de clases mayoritarias, mientras que los métodos de sobremuestreo replican las instancias de las clases minoritarias a partir de las instancias existentes para lograr una distribución de clases equitativa. Los métodos híbridos son alternativas que combinan técnicas de submuestreo y sobremuestreo para equilibrar el conjunto de datos. Con respecto al submuestreo, el submuestreo aleatorio (RU) es un enfoque simple en el que la mayoría de las muestras se seleccionan aleatoriamente para usarlas como datos de entrenamiento. La regla del vecino más cercano editado (ENN) de Wilson [4] elimina una muestra mayoritaria donde dos de sus tres vecinos más cercanos son muestras minoritarias. Tomek Links [5] elimina un par de muestras que pertenecen a diferentes clases y sus vecinos más cercanos. Inverse Random Under Sampling (IRUS) [6] muestra la clase mayoritaria para crear una gran cantidad de conjuntos de entrenamiento distintos que luego podrían encontrar un límite de decisión y separar la clase minoritaria de la clase mayoritaria. En las técnicas de sobremuestreo, la técnica de sobremuestreo de minorías sintéticas (SMOTE) [7] genera muestras sintéticas sin tener en cuenta los ejemplos vecinos. El enfoque de sobremuestreo de recorrido aleatorio (RWO-Sampling) [8] creó muestras sintéticas mediante el recorrido aleatorio de los datos reales y amplió el límite de la clase minoritaria después de que se generaron las muestras sintéticas. Borderline-SMOTE [9] generó las instancias de la clase minoritaria cerca de los límites de la clase. En general, las técnicas de sobremuestreo funcionan mejor que las técnicas de submuestreo con la medición de AUC. Sin embargo, la combinación de ambas técnicas de muestreo obtuvo mejores resultados que la aplicación de submuestreo o sobremuestreo [10][11][12].

Se observó en investigaciones anteriores que la técnica SVM es particularmente sensible al ruido y al valor anormal. Esto puede afectar indirectamente la precisión de la clasificación. Por esta razón, se introdujo FSVM para eliminar el impacto del ruido y el valor anormal [13]. Se han realizado varios estudios aplicando la teoría difusa al clasificador para obtener una mejor tasa de reconocimiento de las clases minoritarias [14][15]. Además, la asignación de diferentes valores de membresía difusos a las instancias de entrenamiento podría manejar problemas de ruido y valores atípicos desequilibrados [16]. En el trabajo previo de los autores, la teoría difusa se combinó con técnicas complementarias denominadas CMTFSVM para eliminar valores atípicos y ruido, y también se comparó la técnica con la técnica CMTNN [17]. Los resultados indicaron que CMTFSVM fue robusto y superó la técnica CMTNN. Por lo tanto, este estudio continúa con la investigación de las técnicas CMTFSVM y SMOTE en datos desequilibrados sesgados y trabajó con un conjunto de datos más grande. El CMTFSVM se aplica como técnica de submuestreo y el SMOTE como técnica de sobremuestreo. Para evaluar el rendimiento de la clasificación de los conjuntos de datos de desequilibrio, la media G y el AUC se utilizan para evaluar el rendimiento del clasificador entre la clase minoritaria y la clase mayoritaria.


1) vectores-difuso-smote.md -> 
2) 11192.md ->
3) 7644.md ->
4) 6838.md ->
5) 1865.md ->
6) 1622.md ->
7) 1042.md ->