# SMOTE para aprender a partir de datos desequilibrados: avances y desafíos, marcando el 15º aniversario

El algoritmo de preprocesamiento de la técnica de sobremuestreo de minorías sintéticas (SMOTE) se considera un estándar "de facto" en el marco del aprendizaje de datos desequilibrados. Esto se debe a su simplicidad en el diseño del procedimiento, así como a su robustez cuando se aplica a diferentes tipos de problemas. Desde su publicación en 2002, SMOTE ha demostrado ser exitoso en una variedad de aplicaciones de varios dominios diferentes. SMOTE también ha inspirado varios enfoques para contrarrestar el problema del desequilibrio de clases y también ha contribuido significativamente a nuevos paradigmas de aprendizaje supervisado, incluida la clasificación de múltiples etiquetas, el aprendizaje incremental, el aprendizaje semisupervisado, el aprendizaje de instancias múltiples, entre otros. Es un punto de referencia estándar para aprender de datos desequilibrados. También se presenta en varios paquetes de software diferentes, desde código abierto hasta comercial. En este documento, que marca el quince aniversario de SMOTE, reflexionamos sobre el viaje de SMOTE, discutimos el estado actual de SMOTE, sus aplicaciones y también identificamos el próximo conjunto de desafíos para extender SMOTE para problemas de Big Data.

1. Introducción 
En la década de 1990, a medida que más datos y aplicaciones de aprendizaje automático y minería de datos comenzaron a prevalecer, surgió un desafío importante: cómo lograr la precisión de clasificación deseada cuando se trata de datos que tenían distribuciones de clase significativamente sesgadas (Sun et al., 2009). ; He & Garcia, 2009; L´opez et al., 2013; Branco et al., 2016; Cieslak et al., 2012; Hoens et al., 2012b; Hoens & Chawla, 2013; Lemaitre et al., 2017; Khan et al., 2018). Autores de varias disciplinas observaron un comportamiento inesperado para los algoritmos de clasificación estándar sobre conjuntos de datos con distribuciones de clase desiguales (Anand, Mehrotra, Mohan y Ranka, 1993; Bruzzone y Serpico, 1997; Kubat, Holte y Matwin, 1998). En muchos casos, la especificidad o precisión local en los ejemplos de las clases mayoritarias superó a la lograda en las minoritarias. Esto condujo al comienzo de un área activa de investigación en aprendizaje automático, ahora denominada "aprendizaje a partir de datos desequilibrados". Fue a principios de la década del 2000 cuando se establecieron las bases del tema durante el primer taller sobre aprendizaje desequilibrado en clase durante la Conferencia de la Asociación Americana de Inteligencia Artificial (Japkowicz & Holte, 2000). El segundo hito se estableció en 2003 durante el Taller ICML-KDD sobre el aprendizaje de conjuntos de datos desequilibrados, lo que dio lugar a una edición especial sobre el tema (Chawla, Japkowicz y Kolcz, 2004). La importancia de esta área de investigación continúa creciendo impulsada en gran medida por las desafiantes declaraciones de problemas de diferentes áreas de aplicación (como el reconocimiento facial, la ingeniería de software, los medios sociales, las redes sociales y el diagnóstico médico), lo que brinda un conjunto novedoso y contemporáneo de desafíos para los investigadores de aprendizaje automático y ciencia de datos (Krawczyk, 2016; Haixiang et al., 2017; Maua & Galinac Grbac, 2017; Zhang et al., 2017; Zuo et al., 2016; Lichtenwalter et al., 2010; Krawczyk et al. ., 2016; Bach et al., 2017; Cao et al., 2017a). La pregunta general que los investigadores han estado tratando de resolver es: ¿cómo ampliar los límites de la predicción en las clases minoritarias o subrepresentadas mientras se maneja la compensación con falsos positivos? El espacio de soluciones ha variado desde enfoques de muestreo hasta nuevos algoritmos de aprendizaje diseñados específicamente para datos desequilibrados. Los enfoques de muestreo se dividen ampliamente en dos grandes categorías: submuestreo o sobremuestreo. Se sabe que las técnicas de submuestreo proporcionan un conjunto de entrenamiento compacto y equilibrado que también reduce el costo de la etapa de aprendizaje. Sin embargo, también conduce a algunos problemas derivados. Primero, aumenta la varianza del clasificador y ii) produce probabilidades posteriores distorsionadas (Dal Pozzolo, Caelen, & Bontempi, 2015). También se pueden descartar algunos ejemplos útiles para el modelado del clasificador. En particular, cuando la relación de desequilibrio es alta, es necesario eliminar más ejemplos, lo que lleva al problema de la falta de datos (Wasikowski y Chen, 2010). Esto puede afectar la capacidad de generalización del clasificador. Como resultado, los investigadores desarrollaron métodos de sobremuestreo que pueden no conducir a la reducción de los ejemplos de la clase mayoritaria y abordan el problema del desequilibrio de clases replicando los ejemplos de la clase minoritaria. Sin embargo, aplicar el sobremuestreo aleatorio solo implica un mayor peso o costo para las instancias minoritarias. Por lo tanto, el modelado correcto de esos grupos de datos minoritarios por el algoritmo de clasificación aún podría ser difícil en el caso de superposición (Garc´ıa, Mollineda, & S´anchez, 2008; Cieslak & Chawla, 2008) o pequeñas disyuntivas (Jo & Japkowicz, 2004). En 2002, Chawla, Bowyer, Hall y Kegelmeyer (2002) propusieron un enfoque novedoso como alternativa al sobremuestreo aleatorio estándar. La idea era superar el sobreajuste generado por el simple sobremuestreo mediante la replicación y ayudar al clasificador a mejorar su generalización en los datos de prueba. En lugar de "ponderar" los puntos de datos, la base de esta nueva técnica de preprocesamiento de datos fue crear nuevas instancias minoritarias. Esta técnica se denominó Técnica de sobremuestreo de minorías sintéticas, ahora ampliamente conocida como SMOTE (Chawla et al., 2002). La base del procedimiento SMOTE fue realizar una interpolación entre instancias vecinas de clases minoritarias. Como tal, puede aumentar el número de instancias de clases minoritarias mediante la introducción de nuevos ejemplos de clases minoritarias en el vecindario, lo que ayuda a los clasificadores a mejorar su capacidad de generalización.

La técnica de preprocesamiento SMOTE se convirtió en pionera para la comunidad de investigación en clasificación desequilibrada. Desde su lanzamiento, se han propuesto muchas extensiones y alternativas para mejorar su desempeño bajo diferentes escenarios. Debido a su popularidad e influencia, SMOTE es considerado como uno de los algoritmos de preprocesamiento/muestreo de datos más influyentes en el aprendizaje automático y la minería de datos (Garc´ıa, Luengo, & Herrera, 2016). Algunos enfoques combinan SMOTE con técnicas de limpieza de datos (Batista, Prati y Monard, 2004). Otros autores se centran en el procedimiento interno modificando algunos de sus componentes, como la selección de las instancias para la generación de nuevos datos (Han, Wang, & Mao, 2005), o el tipo de interpolación (Bunkhumpornpat, Sinapiromsaran, & Lursinsap, 2012 ), entre otros. En este documento, presentamos un resumen de SMOTE y su impacto en los últimos 15 años, celebramos sus contribuciones al aprendizaje automático y la minería de datos, y presentamos el próximo estado de los desafíos para seguir empujando la frontera en el aprendizaje de datos desequilibrados. Si bien no incluimos una discusión sobre las más de 5370 citas de SMOTE (a partir del 1 de febrero de 2018), enfocamos específicamente este documento en enumerar varias extensiones de SMOTE y en discutir el camino a seguir. Por ejemplo, discutimos las extensiones de SMOTE a otros paradigmas de aprendizaje, como transmisión de datos (Krawczyk et al., 2017; Brzezinski & Stefanowski, 2017; Hoens et al., 2011), aprendizaje incremental (Ditzler, Polikar, & Chawla, 2010), deriva de conceptos (Hoens & Chawla, 2012; Hoens, Polikar, & Chawla, 2012a), o tareas de clasificación multi-etiqueta/multi-instancia (Herrera et al., 2016a, 2016b), entre otras. También presentamos un análisis sobre escenarios potenciales dentro de datos desequilibrados que requieren una inmersión más profunda en la aplicación de SMOTE, como las características intrínsecas de los datos (L´opez et al., 2013), incluidas pequeñas disyuntivas, clases superpuestas, etc. Finalmente, planteamos desafíos de clasificación desequilibrada en problemas de Big Data (Fernandez, del Rio, Chawla, & Herrera, 2017). Nuestra esperanza es que este documento proporcione una descripción sumativa de SMOTE, sus extensiones y los desafíos que aún deben abordarse en la comunidad. Este artículo está organizado de la siguiente forma: La sección 2 presenta el algoritmo SMOTE. Luego, la Sección 3 enumera aquellas extensiones al estándar SMOTE que se han propuesto a lo largo de estos años. La Sección 4 presenta el uso de SMOTE bajo diferentes paradigmas de aprendizaje. Los desafíos y temas para el trabajo futuro en algoritmos de preprocesamiento basados ​​en SMOTE se presentan en la Sección 5. Finalmente, la Sección 6 resume y concluye el documento.

### 2. Técnica de sobremuestreo de minorías sintéticas 
En esta sección, primero señalaremos los orígenes del algoritmo SMOTE, estableciendo el contexto bajo el cual fue diseñado (Sección 2.1). Luego, describiremos sus propiedades en detalle para presentar el procedimiento de trabajo de este enfoque de preprocesamiento (Sección 2.2). 
### 2.1 Por qué proponer SMOTE 
Chawla recuerda los orígenes de SMOTE a un problema de clasificación que estaba abordando como estudiante de posgrado en 2000. Estaba trabajando en el desarrollo de un algoritmo de clasificación para aprender y predecir píxeles cancerosos: los datos de mamografía discutidos en el artículo original . Un clasificador de árbol de decisión básico le proporcionó una precisión de alrededor del 97%. Su primera reacción fue de celebración, ya que logró más del 97 % de precisión en un problema que se le presentó como un desafío. Sin embargo, esa celebración duró poco. Rápidamente se dio cuenta de que simplemente adivinando la clase mayoritaria, habría logrado una precisión del 97,68% (que era la distribución de clase mayoritaria en los datos originales). Así que en realidad lo hizo peor que un clasificador de conjetura de clase mayoritaria. Además, el clasificador del árbol de decisiones tuvo un desempeño deficiente en la importante tarea de predecir correctamente las calcificaciones. Esto, por lo tanto, presentó el desafío de: ¿cómo mejorar el desempeño del clasificador en instancias de clase minoritaria?

Un desafío que lo acompañaba era una baja tolerancia a los falsos positivos, es decir, ejemplos de la clase mayoritaria identificada como minoritaria. Es decir, uno tenía que lograr una compensación adecuada entre los verdaderos positivos y los falsos positivos, y no solo ser demasiado agresivo al predecir la clase minoritaria (píxeles cancerosos) para compensar la distribución del 2,32 %. Esto se debió a que había costos asociados con los errores: cada falso negativo acarreaba la carga de clasificar erróneamente un cáncer como no canceroso, y cada falso positivo acarreaba el costo de pruebas adicionales al clasificar erróneamente un no canceroso como cáncer. Los errores claramente no eran de tipos iguales. Chawla probó las herramientas estándar en el arsenal de investigación en ese momento: sobremuestreo por replicación y submuestreo. Ambos enfoques, aunque mejoraron el rendimiento, no proporcionaron resultados satisfactorios. En una investigación más profunda, notó el desafío que surge de sobreajustar las instancias de la clase minoritaria debido al sobremuestreo. Esta observación llevó a la pregunta de: ¿cómo mejorar la capacidad de generalización del clasificador subyacente? Y, por lo tanto, SMOTE se creó para generar sintéticamente nuevas instancias para proporcionar nueva información al algoritmo de aprendizaje para mejorar su previsibilidad sobre las instancias de clase minoritaria. SMOTE proporcionó un rendimiento estadísticamente significativamente superior en los datos de mamografía, así como en varios otros, sentando así las bases para aprender de conjuntos de datos desequilibrados. Por supuesto, SMOTE, al igual que otros enfoques de muestreo, enfrenta el desafío de la cantidad de muestreo, que Chawla y sus colegas también intentaron mitigar mediante el desarrollo de un marco envolvente, similar a la selección de características (Chawla, Cieslak, Hall y Joshi, 2008).

2.2 SMOTE Descripción El algoritmo 

SMOTE lleva a cabo un enfoque de sobremuestreo para reequilibrar el conjunto de entrenamiento original. En lugar de aplicar una simple réplica de las instancias de la clase minoritaria, la idea clave de SMOTE es introducir ejemplos sintéticos. Estos nuevos datos se crean por interpolación entre varias instancias de clases minoritarias que se encuentran dentro de un vecindario definido. Por esta razón, se dice que el procedimiento se centra en el "espacio de características" y no en el "espacio de datos", en otras palabras, el algoritmo se basa en los valores de las características y su relación, en lugar de considerar los puntos de datos. como un todo. Esto también ha llevado a estudiar la relación teórica entre instancias originales y sintéticas que debe ser analizada en profundidad, incluyendo la dimensionalidad de los datos. Se deben considerar algunas propiedades, como la varianza y la correlación en el espacio de datos y características, así como la relación entre el entrenamiento y la distribución de ejemplos de prueba (Blagus & Lusa, 2013). Discutiremos estos temas más adelante en la Sección 5. Un ejemplo simple de SMOTE se ilustra en la Figura 1. Se selecciona una instancia de clase minoritaria xi como base para crear nuevos puntos de datos sintéticos. En función de una métrica de distancia, se eligen varios vecinos más cercanos de la misma clase (puntos xi1 a xi4) del conjunto de entrenamiento. Finalmente, se realiza una interpolación aleatoria para obtener nuevas instancias r1 a r4. El procedimiento formal funciona de la siguiente manera. Primero, se configura la cantidad total de sobremuestreo N (un valor entero), que puede configurarse para obtener una distribución de clases aproximada de 1:1 o descubrirse a través de un proceso de envoltura (Chawla et al., 2008). Luego, se lleva a cabo un proceso iterativo, compuesto por varios pasos. Primero, se selecciona aleatoriamente una instancia de clase minoritaria del conjunto de entrenamiento. A continuación, se obtienen sus K vecinos más cercanos (5 por defecto). Finalmente, N de estas K instancias se eligen aleatoriamente para calcular las nuevas instancias por interpolación. Para ello se toma la diferencia entre el vector de características (muestra) considerado y cada uno de los vecinos seleccionados. Esta diferencia se multiplica por un número aleatorio entre 0 y 1, y luego se suma al vector de características anterior. Esto provoca la selección de un punto aleatorio a lo largo del "segmento de línea" entre las características. En el caso de atributos nominales, se selecciona al azar uno de los dos valores. Todo el proceso se resume en el Algoritmo 1.

```
function SMOTE(T, N, k)
1: Input: T; N; k . #ejemplos de clases minoritarias, cantidad de sobremuestreo, #vecinos más cercanos
Output: (N/100) * T muestras sintéticas de clase minoritaria
Variables: Sample[][]: matriz para muestras originales de clases minoritarias;
newindex: mantiene un recuento del número de muestras sintéticas generadas, initialized to 0;
Synthetic[][]: matriz para muestras sintéticas
2: if N < 100 then
3:      Aleatorizar las muestras de la clase minoritaria T
4:      T = (N/100)*T
5:      N = 100
6: end if
7: N = (int)N/100 . Se supone que la cantidad de SMOTE está en múltiplos enteros de 100.
8: for i = 1 to T do
9:      Calcule k vecinos más cercanos para i, y guarde los índices en el nnarray
10:     POPULATE(N, i, nnarray)
11: end for
12: end function
```

```
1: function POPULATE(N, i, nnarray)
Input: N; i; nnarray . #instances to create, original sample index, array of nearest
neighbors
Output: N new synthetic samples in Synthetic array
2: while N 6= 0 do
3: nn = random(1,k)
4: for attr = 1 to numattrs do . numattrs = Number of attributes
5: Compute: dif = Sample[nnarray[nn]][attr] − Sample[i][attr]
6: Compute: gap = random(0, 1)
7: Synthetic[newindex][attr] = Sample[i][attr] + gap · dif
8: end for
9: newindex + +
10: N − −
11: end while
12: end function
```