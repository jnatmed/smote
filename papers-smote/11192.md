# SMOTE para aprender a partir de datos desequilibrados: avances y desafíos, marcando el 15º aniversario

El algoritmo de preprocesamiento de la técnica de sobremuestreo de minorías sintéticas (SMOTE) se considera un estándar "de facto" en el marco del aprendizaje de datos desequilibrados. Esto se debe a su simplicidad en el diseño del procedimiento, así como a su robustez cuando se aplica a diferentes tipos de problemas. Desde su publicación en 2002, SMOTE ha demostrado ser exitoso en una variedad de aplicaciones de varios dominios diferentes. SMOTE también ha inspirado varios enfoques para contrarrestar el problema del desequilibrio de clases y también ha contribuido significativamente a nuevos paradigmas de aprendizaje supervisado, incluida la clasificación de múltiples etiquetas, el aprendizaje incremental, el aprendizaje semisupervisado, el aprendizaje de instancias múltiples, entre otros. Es un punto de referencia estándar para aprender de datos desequilibrados. También se presenta en varios paquetes de software diferentes, desde código abierto hasta comercial. En este documento, que marca el quince aniversario de SMOTE, reflexionamos sobre el viaje de SMOTE, discutimos el estado actual de SMOTE, sus aplicaciones y también identificamos el próximo conjunto de desafíos para extender SMOTE para problemas de Big Data.

1. Introducción 
En la década de 1990, a medida que más datos y aplicaciones de aprendizaje automático y minería de datos comenzaron a prevalecer, surgió un desafío importante: cómo lograr la precisión de clasificación deseada cuando se trata de datos que tenían distribuciones de clase significativamente sesgadas (Sun et al., 2009). ; He & Garcia, 2009; L´opez et al., 2013; Branco et al., 2016; Cieslak et al., 2012; Hoens et al., 2012b; Hoens & Chawla, 2013; Lemaitre et al., 2017; Khan et al., 2018). Autores de varias disciplinas observaron un comportamiento inesperado para los algoritmos de clasificación estándar sobre conjuntos de datos con distribuciones de clase desiguales (Anand, Mehrotra, Mohan y Ranka, 1993; Bruzzone y Serpico, 1997; Kubat, Holte y Matwin, 1998). En muchos casos, la especificidad o precisión local en los ejemplos de las clases mayoritarias superó a la lograda en las minoritarias. Esto condujo al comienzo de un área activa de investigación en aprendizaje automático, ahora denominada "aprendizaje a partir de datos desequilibrados". Fue a principios de la década del 2000 cuando se establecieron las bases del tema durante el primer taller sobre aprendizaje desequilibrado en clase durante la Conferencia de la Asociación Americana de Inteligencia Artificial (Japkowicz & Holte, 2000). El segundo hito se estableció en 2003 durante el Taller ICML-KDD sobre el aprendizaje de conjuntos de datos desequilibrados, lo que dio lugar a una edición especial sobre el tema (Chawla, Japkowicz y Kolcz, 2004). La importancia de esta área de investigación continúa creciendo impulsada en gran medida por las desafiantes declaraciones de problemas de diferentes áreas de aplicación (como el reconocimiento facial, la ingeniería de software, los medios sociales, las redes sociales y el diagnóstico médico), lo que brinda un conjunto novedoso y contemporáneo de desafíos para los investigadores de aprendizaje automático y ciencia de datos (Krawczyk, 2016; Haixiang et al., 2017; Maua & Galinac Grbac, 2017; Zhang et al., 2017; Zuo et al., 2016; Lichtenwalter et al., 2010; Krawczyk et al. ., 2016; Bach et al., 2017; Cao et al., 2017a). La pregunta general que los investigadores han estado tratando de resolver es: ¿cómo ampliar los límites de la predicción en las clases minoritarias o subrepresentadas mientras se maneja la compensación con falsos positivos? El espacio de soluciones ha variado desde enfoques de muestreo hasta nuevos algoritmos de aprendizaje diseñados específicamente para datos desequilibrados. Los enfoques de muestreo se dividen ampliamente en dos grandes categorías: submuestreo o sobremuestreo. Se sabe que las técnicas de submuestreo proporcionan un conjunto de entrenamiento compacto y equilibrado que también reduce el costo de la etapa de aprendizaje. Sin embargo, también conduce a algunos problemas derivados. Primero, aumenta la varianza del clasificador y ii) produce probabilidades posteriores distorsionadas (Dal Pozzolo, Caelen, & Bontempi, 2015). También se pueden descartar algunos ejemplos útiles para el modelado del clasificador. En particular, cuando la relación de desequilibrio es alta, es necesario eliminar más ejemplos, lo que lleva al problema de la falta de datos (Wasikowski y Chen, 2010). Esto puede afectar la capacidad de generalización del clasificador. Como resultado, los investigadores desarrollaron métodos de sobremuestreo que pueden no conducir a la reducción de los ejemplos de la clase mayoritaria y abordan el problema del desequilibrio de clases replicando los ejemplos de la clase minoritaria. Sin embargo, aplicar el sobremuestreo aleatorio solo implica un mayor peso o costo para las instancias minoritarias. Por lo tanto, el modelado correcto de esos grupos de datos minoritarios por el algoritmo de clasificación aún podría ser difícil en el caso de superposición (Garc´ıa, Mollineda, & S´anchez, 2008; Cieslak & Chawla, 2008) o pequeñas disyuntivas (Jo & Japkowicz, 2004). En 2002, Chawla, Bowyer, Hall y Kegelmeyer (2002) propusieron un enfoque novedoso como alternativa al sobremuestreo aleatorio estándar. La idea era superar el sobreajuste generado por el simple sobremuestreo mediante la replicación y ayudar al clasificador a mejorar su generalización en los datos de prueba. En lugar de "ponderar" los puntos de datos, la base de esta nueva técnica de preprocesamiento de datos fue crear nuevas instancias minoritarias. Esta técnica se denominó Técnica de sobremuestreo de minorías sintéticas, ahora ampliamente conocida como SMOTE (Chawla et al., 2002). La base del procedimiento SMOTE fue realizar una interpolación entre instancias vecinas de clases minoritarias. Como tal, puede aumentar el número de instancias de clases minoritarias mediante la introducción de nuevos ejemplos de clases minoritarias en el vecindario, lo que ayuda a los clasificadores a mejorar su capacidad de generalización.

La técnica de preprocesamiento SMOTE se convirtió en pionera para la comunidad de investigación en clasificación desequilibrada. Desde su lanzamiento, se han propuesto muchas extensiones y alternativas para mejorar su desempeño bajo diferentes escenarios. Debido a su popularidad e influencia, SMOTE es considerado como uno de los algoritmos de preprocesamiento/muestreo de datos más influyentes en el aprendizaje automático y la minería de datos (Garc´ıa, Luengo, & Herrera, 2016). Algunos enfoques combinan SMOTE con técnicas de limpieza de datos (Batista, Prati y Monard, 2004). Otros autores se centran en el procedimiento interno modificando algunos de sus componentes, como la selección de las instancias para la generación de nuevos datos (Han, Wang, & Mao, 2005), o el tipo de interpolación (Bunkhumpornpat, Sinapiromsaran, & Lursinsap, 2012 ), entre otros. En este documento, presentamos un resumen de SMOTE y su impacto en los últimos 15 años, celebramos sus contribuciones al aprendizaje automático y la minería de datos, y presentamos el próximo estado de los desafíos para seguir empujando la frontera en el aprendizaje de datos desequilibrados. Si bien no incluimos una discusión sobre las más de 5370 citas de SMOTE (a partir del 1 de febrero de 2018), enfocamos específicamente este documento en enumerar varias extensiones de SMOTE y en discutir el camino a seguir. Por ejemplo, discutimos las extensiones de SMOTE a otros paradigmas de aprendizaje, como transmisión de datos (Krawczyk et al., 2017; Brzezinski & Stefanowski, 2017; Hoens et al., 2011), aprendizaje incremental (Ditzler, Polikar, & Chawla, 2010), deriva de conceptos (Hoens & Chawla, 2012; Hoens, Polikar, & Chawla, 2012a), o tareas de clasificación multi-etiqueta/multi-instancia (Herrera et al., 2016a, 2016b), entre otras. También presentamos un análisis sobre escenarios potenciales dentro de datos desequilibrados que requieren una inmersión más profunda en la aplicación de SMOTE, como las características intrínsecas de los datos (L´opez et al., 2013), incluidas pequeñas disyuntivas, clases superpuestas, etc. Finalmente, planteamos desafíos de clasificación desequilibrada en problemas de Big Data (Fernandez, del Rio, Chawla, & Herrera, 2017). Nuestra esperanza es que este documento proporcione una descripción sumativa de SMOTE, sus extensiones y los desafíos que aún deben abordarse en la comunidad. Este artículo está organizado de la siguiente forma: La sección 2 presenta el algoritmo SMOTE. Luego, la Sección 3 enumera aquellas extensiones al estándar SMOTE que se han propuesto a lo largo de estos años. La Sección 4 presenta el uso de SMOTE bajo diferentes paradigmas de aprendizaje. Los desafíos y temas para el trabajo futuro en algoritmos de preprocesamiento basados ​​en SMOTE se presentan en la Sección 5. Finalmente, la Sección 6 resume y concluye el documento.

### 2. Técnica de sobremuestreo de minorías sintéticas 
En esta sección, primero señalaremos los orígenes del algoritmo SMOTE, estableciendo el contexto bajo el cual fue diseñado (Sección 2.1). Luego, describiremos sus propiedades en detalle para presentar el procedimiento de trabajo de este enfoque de preprocesamiento (Sección 2.2). 
### 2.1 Por qué proponer SMOTE 
Chawla recuerda los orígenes de SMOTE a un problema de clasificación que estaba abordando como estudiante de posgrado en 2000. Estaba trabajando en el desarrollo de un algoritmo de clasificación para aprender y predecir píxeles cancerosos: los datos de mamografía discutidos en el artículo original . Un clasificador de árbol de decisión básico le proporcionó una precisión de alrededor del 97%. Su primera reacción fue de celebración, ya que logró más del 97 % de precisión en un problema que se le presentó como un desafío. Sin embargo, esa celebración duró poco. Rápidamente se dio cuenta de que simplemente adivinando la clase mayoritaria, habría logrado una precisión del 97,68% (que era la distribución de clase mayoritaria en los datos originales). Así que en realidad lo hizo peor que un clasificador de conjetura de clase mayoritaria. Además, el clasificador del árbol de decisiones tuvo un desempeño deficiente en la importante tarea de predecir correctamente las calcificaciones. Esto, por lo tanto, presentó el desafío de: ¿cómo mejorar el desempeño del clasificador en instancias de clase minoritaria?

Un desafío que lo acompañaba era una baja tolerancia a los falsos positivos, es decir, ejemplos de la clase mayoritaria identificada como minoritaria. Es decir, uno tenía que lograr una compensación adecuada entre los verdaderos positivos y los falsos positivos, y no solo ser demasiado agresivo al predecir la clase minoritaria (píxeles cancerosos) para compensar la distribución del 2,32 %. Esto se debió a que había costos asociados con los errores: cada falso negativo acarreaba la carga de clasificar erróneamente un cáncer como no canceroso, y cada falso positivo acarreaba el costo de pruebas adicionales al clasificar erróneamente un no canceroso como cáncer. Los errores claramente no eran de tipos iguales. Chawla probó las herramientas estándar en el arsenal de investigación en ese momento: sobremuestreo por replicación y submuestreo. Ambos enfoques, aunque mejoraron el rendimiento, no proporcionaron resultados satisfactorios. En una investigación más profunda, notó el desafío que surge de sobreajustar las instancias de la clase minoritaria debido al sobremuestreo. Esta observación llevó a la pregunta de: ¿cómo mejorar la capacidad de generalización del clasificador subyacente? Y, por lo tanto, SMOTE se creó para generar sintéticamente nuevas instancias para proporcionar nueva información al algoritmo de aprendizaje para mejorar su previsibilidad sobre las instancias de clase minoritaria. SMOTE proporcionó un rendimiento estadísticamente significativamente superior en los datos de mamografía, así como en varios otros, sentando así las bases para aprender de conjuntos de datos desequilibrados. Por supuesto, SMOTE, al igual que otros enfoques de muestreo, enfrenta el desafío de la cantidad de muestreo, que Chawla y sus colegas también intentaron mitigar mediante el desarrollo de un marco envolvente, similar a la selección de características (Chawla, Cieslak, Hall y Joshi, 2008).

2.2 SMOTE Descripción El algoritmo 

SMOTE lleva a cabo un enfoque de sobremuestreo para reequilibrar el conjunto de entrenamiento original. En lugar de aplicar una simple réplica de las instancias de la clase minoritaria, la idea clave de SMOTE es introducir ejemplos sintéticos. Estos nuevos datos se crean por interpolación entre varias instancias de clases minoritarias que se encuentran dentro de un vecindario definido. Por esta razón, se dice que el procedimiento se centra en el "espacio de características" y no en el "espacio de datos", en otras palabras, el algoritmo se basa en los valores de las características y su relación, en lugar de considerar los puntos de datos. como un todo. Esto también ha llevado a estudiar la relación teórica entre instancias originales y sintéticas que debe ser analizada en profundidad, incluyendo la dimensionalidad de los datos. Se deben considerar algunas propiedades, como la varianza y la correlación en el espacio de datos y características, así como la relación entre el entrenamiento y la distribución de ejemplos de prueba (Blagus & Lusa, 2013). Discutiremos estos temas más adelante en la Sección 5. Un ejemplo simple de SMOTE se ilustra en la Figura 1. Se selecciona una instancia de clase minoritaria xi como base para crear nuevos puntos de datos sintéticos. En función de una métrica de distancia, se eligen varios vecinos más cercanos de la misma clase (puntos xi1 a xi4) del conjunto de entrenamiento. Finalmente, se realiza una interpolación aleatoria para obtener nuevas instancias r1 a r4. El procedimiento formal funciona de la siguiente manera. Primero, se configura la cantidad total de sobremuestreo N (un valor entero), que puede configurarse para obtener una distribución de clases aproximada de 1:1 o descubrirse a través de un proceso de envoltura (Chawla et al., 2008). Luego, se lleva a cabo un proceso iterativo, compuesto por varios pasos. Primero, se selecciona aleatoriamente una instancia de clase minoritaria del conjunto de entrenamiento. A continuación, se obtienen sus K vecinos más cercanos (5 por defecto). Finalmente, N de estas K instancias se eligen aleatoriamente para calcular las nuevas instancias por interpolación. Para ello se toma la diferencia entre el vector de características (muestra) considerado y cada uno de los vecinos seleccionados. Esta diferencia se multiplica por un número aleatorio entre 0 y 1, y luego se suma al vector de características anterior. Esto provoca la selección de un punto aleatorio a lo largo del "segmento de línea" entre las características. En el caso de atributos nominales, se selecciona al azar uno de los dos valores. Todo el proceso se resume en el Algoritmo 1.

```
function SMOTE(T, N, k)
1: Input: T; N; k . #ejemplos de clases minoritarias, cantidad de sobremuestreo, #vecinos más cercanos
Output: (N/100) * T muestras sintéticas de clase minoritaria
Variables: Sample[][]: matriz para muestras originales de clases minoritarias;
newindex: mantiene un recuento del número de muestras sintéticas generadas, initialized to 0;
Synthetic[][]: matriz para muestras sintéticas
2: if N < 100 then
3:      Aleatorizar las muestras de la clase minoritaria T
4:      T = (N/100)*T
5:      N = 100
6: end if
7: N = (int)N/100 . Se supone que la cantidad de SMOTE está en múltiplos enteros de 100.
8: for i = 1 to T do
9:      Calcule k vecinos más cercanos para i, y guarde los índices en el nnarray
10:     POPULATE    (N, i, nnarray)
11: end for
12: end function
```

```
1: function POPULATE(N, i, nnarray)
Input: N; i; nnarray . #instancias para crear, índice de muestra original, matriz de vecinos más cercanos
Output: N new synthetic samples in Synthetic array
2: while N 6= 0 do
3:      nn = random(1,k)
4:      for attr = 1 to numattrs do . numattrs = Numero de Atributos
5:          Compute: dif = Sample[nnarray[nn]][attr] − Sample[i][attr]
6:          Compute: gap = random(0, 1)
7:          Synthetic[newindex][attr] = Sample[i][attr] + gap · dif
8:      end for
9:      newindex++
10:     N−−
11: end while
12: end function
```
Considere una muestra (6,4) y sea (4,3) su vecino más cercano. (6,4) es la muestra para la que se identifican los k-vecinos más cercanos (4,3) es uno de sus k-vecinos más cercanos. Sea: 
f1_1 = 6 f2_1 = 4, 
f2_1 - f1_1 = -2 
f1_2 = 4 f2_2 = 3, 
f2_2 - f1_2 = -1 
Las nuevas muestras se generarán como f1',f2' = (6,4) + rand(0 -1) * (-2,-1) rand(0-1) genera un vector de dos números aleatorios entre 0 y 1.

La figura 2 muestra un ejemplo simple de la aplicación SMOTE para comprender cómo se calculan las instancias sintéticas. Para concluir esta sección, nuestro objetivo es presentar algunas de las primeras aplicaciones reales que hicieron un uso exitoso del algoritmo de preprocesamiento SMOTE, ambas basadas en el área de Bioinformática. Específicamente, enfatizamos un problema de múltiples clases de funciones moleculares de proteínas de levadura (Hwang, Fotouhi, Finley Jr., & Grosky, 2003). El problema original se dividió en subconjuntos binarios desequilibrados, por lo que se necesitaban nuevas instancias sintéticas antes de la etapa de aprendizaje de una red neuronal modular para evitar el sesgo hacia las clases mayoritarias.

### 3. Extensiones de SMOTE 

A continuación, presentamos los enfoques basados ​​en SMOTE más significativos propuestos en los últimos 15 años y un conjunto de propiedades comunes compartidas por ellos. Consideramos SMOTE como una base para el sobremuestreo con generación artificial de instancias de clase minoritaria. Por ello, entendemos que cualquier método de preprocesamiento en el ámbito de la clasificación desequilibrada que se base en la creación sintética de ejemplos mediante cualquier tipo de interpolación u otro proceso tiene algún grado de relación con el algoritmo SMOTE original. Primero, en la Sección 3.1, se describirán las características esenciales. A continuación, en la Sección 3.2, enumeraremos todas las extensiones basadas en SMOTE propuestas en la literatura científica hasta el momento. Luego, cada método se categorizará de acuerdo con las propiedades estudiadas para proporcionar una taxonomía completa. A continuación, en la Sección 3.3 presentaremos una lista de multiclasificadores basados ​​en SMOTE propuestos junto con su categorización. Finalmente, la Sección 3.4 describirá los estudios experimentales más influyentes presentados en la literatura que involucran SMOTE como punto clave. 

#### 3.1 Propiedades para clasificar las extensiones basadas en SMOTE
 
Esta sección proporciona un marco para la organización de las extensiones basadas en SMOTE que se presentarán en las secciones 3.2 y 3.3. Los aspectos discutidos aquí consisten en (1) selección inicial de instancias para ser sobremuestreadas, (2) integración con Undersampling como paso en la técnica, (3) tipo de interpolación, (4) operación con cambios de dimensionalidad, (5) generación adaptativa de ejemplos sintéticos, (6) posibilidad de reetiquetado y (7) filtrado de instancias generadas con ruido. Estas facetas mencionadas están involucradas en la definición de la categorización, pues determinan el modo de operación de cada técnica. A continuación, describimos en detalle cada propiedad.

• Selección inicial de instancias a sobremuestrear: Es habitual determinar los mejores candidatos a sobremuestrear en los datos antes de que comience el proceso de generación de ejemplos sintéticos. Esta estrategia está destinada a reducir la superposición y el ruido en el conjunto de datos final. Muchas técnicas optan por elegir las instancias cercanas a las clases límite (Han et al., 2005) o por no generar un ejemplo sintético en función del número de ejemplos de clases minoritarias pertenecientes al barrio (Bunkhumpornpat, Sinapiromsaran, & Lursinsap, 2009). Aunque en la literatura se han propuesto muchas alternativas de selección inicial, casi todas siguen alguna de las dos estrategias mencionadas. Dos excepciones son la generación de ejemplos sintéticos tras un proceso de optimización de LVQ (Nakamura, Kajiwara, Otsuka, & Kimura, 2013) y la selección de puntos iniciales a partir de los vectores soporte obtenidos por un SVM (Cervantes, Garc´ıa-Lamont, Rodr´ Íguez-Mazahua, Chau, Ruiz-Castilla, & Trueba, 2017). 

• Integración con Undersampling: Los ejemplos pertenecientes a la clase mayoritaria también se eliminan utilizando una técnica de undersampling aleatoria o informada. El paso de submuestreo se puede realizar al comienzo del sobremuestreo o como una operación interna junto con la generación de ejemplos sintéticos. Generalmente, el sobremuestreo sigue al submuestreo. 

• Tipo de interpolación: Esta propiedad ofrece variados mecanismos sobre la generación de ejemplos artificiales o sintéticos y frecuentemente se asocia con la principal originalidad del desarrollo de la novela. Define la forma en que se crean nuevos ejemplos artificiales y se pueden encontrar muchas alternativas. Los mecanismos de interpolación pueden tener un rango restringido (Han et al., 2005; Bunkhumpornpat et al., 2009; Maciejewski & Stefanowski, 2011), por ejemplo, buscando no solo a los vecinos más cercanos de la clase minoritaria sino también de la clase mayoritaria; crear nuevos ejemplos más cercanos a la instancia seleccionada que su vecino o mediante el uso de la ponderación de características (Hukerikar, Tumma, Nikam y Attar, 2011); múltiples interpolaciones (de la Calleja & Fuentes, 2007; Gazzah & Amara, 2008) que involucran más de dos ejemplos o siguen topologías basadas en formas geométricas, como elipses (Abdi & Hashemi, 2016) y diagramas de voronoi (Young, Nykl, Weckman, & Chelberg, 2015), y gráficos (Bunkhumpornpat et al., 2012); interpolación basada en clustering (Barua, Islam, & Murase, 2011), en la que los nuevos ejemplos pueden ser los centroides del cluster o pueden crearse con ejemplos que pertenecen al mismo cluster; interpolaciones que utilizan distintas distribuciones aleatorias, como la gaussiana (Sandhan & Choi, 2014), estimaciones de la función de distribución de probabilidad de los datos (Gao, Hong, Chen, Harris, & Khalaf, 2014b), suavizamiento de probabilidad (Wang, Li, Chao, & Cao, 2012), preservación de covarianzas (Cateni, Colla, & Vannucci, 2011) entre datos e interpolaciones más complejas, como cadenas de Markov (Das, Krishnan, & Cook, 2015) o uniones Q (Rong, Gong , y Ng, 2014). Incluso es posible no tener interpolación, como cuando los nuevos datos se generan usando un solo punto, a través de jittering (Mease, Wyner, & Buja, 2007), perturbaciones gaussianas (de la Calleja, Fuentes, & Gonz´alez, 2008), simples copias con cambios de etiqueta (Stefanowski & Wilk, 2008) o incluso combinando sobremuestreo con empujar las muestras mayoritarias fuera de una esfera (Koziarski, Krawczyk, & Wozniak, 2017).

• Operación con cambios de dimensionalidad: Esto ocurre cuando la técnica incorpora ya sea una reducción o aumento de dimensionalidad antes o durante la generación de ejemplos artificiales o sintéticos. El enfoque más común es cambiar la dimensionalidad de los datos al principio y luego trabajar en el nuevo espacio dimensional; ya sea reduciéndolo a través del Análisis de Componentes Principales (PCA) (Abdi & Hashemi, 2016) o técnicas relacionadas (Gu, Cai, & Zhu, 2009; Xie, Jiang, Ye, & Li, 2015), selección de características (Koto, 2014) , embolsado (Wang, Yun, li Huang y ao Liu, 2013a), técnicas múltiples (Bellinger, Drummond y Japkowicz, 2016) y codificadores automáticos (Bellinger, Japkowicz y Drummond, 2015), y mediante el uso de funciones del kernel ( Mathew, Luo, Pang, & Chan, 2015; Tang & He, 2015; P´erez-Ortiz, Guti´errez, Ti˜no, & Herv´as-Mart´ınez, 2016). Además, se puede utilizar una estimación de los componentes principales de los datos para conducir la interpolación (Tang & Chen, 2008). 

• Generación adaptativa de ejemplos sintéticos: La hipótesis de generación adaptativa, ADASYN (He, Bai, Garcia, & Li, 2008), era utilizar una distribución ponderada en función de cada ejemplo de clase minoritaria según su grado de dificultad a la hora de aprender. De esta forma, se generarán más datos sintéticos para algunas instancias de clases minoritarias que son más complicadas de aprender en comparación con otras. Inspiradas en ADASYN, muchas técnicas incorporan mecanismos similares para controlar la cantidad de nuevos ejemplos artificiales a generar asociados a cada ejemplo minoritario o subgrupos de ejemplos minoritarios (Alejo, Garc´ıa, & Pacheco-S´anchez, 2015; Rivera, 2017). ). 

• Reetiquetado: La técnica ofrece la opción de reetiquetar los ejemplos pertenecientes a la clase mayoritaria durante la generación sintética de ejemplos (Dang, Tran, Hirose, & Satou, 2015) o reemplazando el mecanismo de interpolación (Blaszczynski, Deckert, Stefanowski, & Wilk, 2012). 

• Filtrado de instancias generadas con ruido: Las primeras extensiones de SMOTE motivadas por su conocido inconveniente de generar ejemplos superpuestos y ruidosos fue la adición de un paso de filtrado de ruido justo después de que finaliza el proceso de SMOTE. Dos técnicas típicas son SMOTE-TomekLinks y SMOTE+ENN (Batista et al., 2004). El filtrado de ejemplos artificiales es una operación frecuente que respalda el éxito de SMOTE en datos reales.
Se han propuesto muchos tipos de filtros para mejorar SMOTE, como estrategias de filtrado codicioso (Puntumapon & Waiyamai, 2012), filtrado basado en conjuntos aproximados (Ramentol, Caballero, Bello, & Herrera, 2012; Hu & Li, 2013; Ramentol, Gondres , Lajes, Bello, Caballero, Cornelis, & Herrera, 2016), filtrado basado en conjuntos (S´aez, Luengo, Stefanowski, & Herrera, 2015) y procedimientos de optimización bioinspirados (L´opez, Triguero, Carmona, Garc´ıa, & Herrera, 2014; Zieba, Tomczak, & Gonczarek, 2015; Jiang, Lu, & Xia, 2016; Cervantes et al., 2017).

#### 3.2 Extensiones basadas en SMOTE para sobremuestreo 
Hasta la fecha, se han propuesto más de 85 extensiones de SMOTE en la literatura especializada. Esta sección está dedicada a enumerarlos y categorizarlos según las propiedades estudiadas anteriormente. La Tabla 1 presenta una enumeración de los métodos revisados ​​en este documento. En este campo, es habitual que los autores den un nombre a su propuesta, con algunas excepciones. Como podemos ver en la Tabla 1, las propiedades más frecuentemente explotadas por las técnicas son la selección inicial y la generación adaptativa de ejemplos sintéticos. El filtrado se está volviendo más común en los últimos años, así como el uso de funciones del kernel. En cuanto al procedimiento de interpolación, también es habitual sustituir el método original por otros más complejos, como los basados ​​en clustering o derivados de una función probabilística. Cabe mencionar que no existe una técnica que aplique los cuatro mecanismos pertinentes a la calibración de la generación de ejemplos artificiales, selección y remoción de ejemplos nocivos ya sean sintéticos o pertenecientes a la clase mayoritaria; a saber, selección inicial, integración con submuestreo, generación adaptativa y filtrado en conjunto. Debido a limitaciones de espacio, no es posible describir todas las técnicas revisadas. No obstante, proporcionaremos breves explicaciones de las técnicas más conocidas de la Tabla 1: 

• Limite-SMOTE (Han et al., 2005): Este algoritmo se basa en la premisa de que los ejemplos alejados del límite pueden contribuir poco a la éxito de la clasificación. Por lo tanto, la técnica identifica aquellos ejemplos que pertenecen al límite utilizando la relación entre los ejemplos mayoritarios y minoritarios dentro de la vecindad de cada instancia que se va a sobremuestrear. No se consideran ejemplos ruidosos, los que tienen todos los vecinos de la clase mayoritaria. Los llamados ejemplos peligrosos, con una proporción adecuada, se sobremuestrean. 

• AHC (Cohen et al., 2006): Fue el primer intento de utilizar el agrupamiento para generar nuevos ejemplos sintéticos para equilibrar los datos. El algoritmo K-means se usó para submuestrear los ejemplos mayoritarios y el agrupamiento jerárquico aglomerativo se usó para sobremuestrear los ejemplos minoritarios. Aquí, los grupos se recopilan de todos los niveles de los dendogramas resultantes y sus centroides se interpolan con los ejemplos originales de clases minoritarias. 

• ADASYN (He et al., 2008): Su idea principal parte del supuesto de utilizar una distribución ponderada según el tipo de ejemplos minoritarios según su complejidad para el aprendizaje. La cantidad de datos sintéticos de cada uno está asociada al nivel de dificultad de cada ejemplo minoritario. Esta estimación de dificultad se basa en la proporción de ejemplos pertenecientes a la clase mayoritaria del vecindario. Luego, se calcula una distribución de densidad usando todas las proporciones de las instancias minoritarias, que se usarán para calcular la cantidad de ejemplos sintéticos necesarios para generar para cada ejemplo minoritario.

• Safe-Level-SMOTE (Bunkhumpornpat et al., 2009): Asigna a cada ejemplo minoritario un nivel seguro antes de generar instancias sintéticas. Cada instancia sintética se posicionará más cerca del nivel seguro más grande, generando así todas las instancias sintéticas solo en regiones seguras. El nivel seguro es la relación entre el número de ejemplos minoritarios dentro del vecindario y la relación del nivel seguro depende del nivel seguro de cada instancia y el de los ejemplos en su vecindario. La interpolación está controlada por una brecha que depende de la relación de nivel seguro de cada instancia minoritaria. 

• DBSMOTE (Bunkhumpornpat et al., 2012): este algoritmo se basa en un enfoque de agrupación en clústeres basado en la densidad denominado DBSCAN y realiza un sobremuestreo generando muestras sintéticas a lo largo de la ruta más corta desde cada instancia minoritaria hasta un pseudocentroide de un clúster de clase minoritaria. DBSMOTE se inspiró en Borderline-SMOTE en el sentido de que opera en una región superpuesta, pero a diferencia de Borderline-SMOTE, también intenta mantener las precisiones de las clases mayoritaria y minoritaria. 

• ROSE (Menardi & Torelli, 2014): ROSE es una técnica de sobremuestreo propuesta dentro de un marco completo para obtener reglas de clasificación en datos desbalanceados. Se establece a partir de la generación de nuevos datos artificiales a partir de las clases, según una forma de bootstrap suavizada y la idea que hay detrás se sustenta en las conocidas propiedades teóricas de los métodos kernel. El algoritmo muestrea una nueva instancia usando la distribución de probabilidad centrada en un ejemplo seleccionado al azar y dependiendo de una matriz suavizante de parámetros de escala. 

• MWMOTE (Barua et al., 2014): Basado en el supuesto de que los métodos de sobremuestreo existentes pueden generar muestras minoritarias sintéticas incorrectas, MWMOTE analiza los ejemplos minoritarios más difíciles y les asigna un peso de acuerdo con su distancia de los ejemplos mayoritarios más cercanos. Luego, los ejemplos sintéticos se generan a partir de la instancia de clase minoritaria informativa ponderada utilizando un enfoque de agrupación, lo que garantiza que deben estar dentro de un grupo de clase minoritaria. 

• MDO (Abdi & Hashemi, 2016): Es uno de los enfoques multiclase recientes inspirados en la distancia de Mahalanobis. MDO construye ejemplos sintéticos que tienen la misma distancia de Mahalanobis de cada clase examinada que los otros ejemplos minoritarios. Por lo tanto, la región de instancias minoritarias se puede aprender mejor conservando la covarianza durante la generación de ejemplos sintéticos a lo largo de los contornos de probabilidad. Además, se reduce el riesgo de superposición entre diferentes regiones de clase.

3.3 Extensiones basadas en SMOTE para conjuntos 

Los conjuntos de clasificadores han surgido como un marco de aprendizaje popular para abordar problemas de clasificación desequilibrada. SMOTE también se ha involucrado y/o se ha extendido a muchos métodos basados ​​en conjuntos. La Tabla 2 muestra una lista de técnicas basadas en conjuntos que incorporan SMOTE en sí mismo o un derivado de SMOTE como un paso importante para lograr la diversidad del conjunto de clasificadores aprendidos para formar el conjunto. Tenga en cuenta que esta tabla solo contiene los métodos relacionados con el sobremuestreo y la generación de ejemplos sintéticos; el lector puede consultar la literatura especializada para revisar otros conjuntos propuestos para el aprendizaje desequilibrado en los que SMOTE no participa (Galar, Fernandez, Barrenechea, Bustince, & Herrera, 2012; Fern´andez, L´opez, Galar, Del Jesus, & Herrera, 2013; Hoens & Chawla, 2010). Específicamente, es importante señalar que podemos encontrar varios estudios que muestran un buen comportamiento para los enfoques basados ​​en submuestreo en sinergia con el aprendizaje conjunto (Khoshgoftaar, Hulse, & Napolitano, 2011; Galar et al., 2012; Blaszczynski & Stefanowski , 2015; Galar, Fernández, Barrenechea, Bustince, & Herrera, 2016). La estructura de la Tabla 2 es muy similar a la anterior, la Tabla 1. El cambio de dimensionalidad y el filtrado son dos propiedades que no se utilizan en los conjuntos. Además, agregamos una nueva columna que designa el tipo de método de conjunto, es decir, si el método es un enfoque de refuerzo, embolsado o uno contra todos (OVA). El resto de propiedades se explican en el apartado 3.1.

3.4 Estudios empíricos exhaustivos que involucran SMOTE 

SMOTE se establece como el estándar o punto de referencia "de facto" en el aprendizaje de conjuntos de datos desequilibrados. Aunque sería imposible examinar todos los estudios analíticos que involucran SMOTE en cualquier paso, en esta breve sección, revisamos algunos de los estudios empíricos más influyentes que estudiaron SMOTE en profundidad. El primer tipo de estudios experimentales surgió para comprobar si el sobremuestreo es más eficaz que el submuestreo y qué tasa de sobremuestreo o submuestreo se debe utilizar (Estabrooks, Jo, & Japkowicz, 2004). Varios estudios abordaron este tema desde un punto de vista más general (L´opez et al., 2013) ***y se centraron específicamente en SMOTE para preguntar cómo descubrir la cantidad y el tipo de muestreo adecuados (Chawla et al., 2008)***. En el trabajo de Batista et al. (2004), se comparan algunos enfoques comunes de remuestreo y se demostró que las hibridaciones de SMOTE con submuestreo superan al resto de técnicas de remuestreo. Posteriormente, Prati, Batista y Silva (2015) diseñaron un montaje experimental renovado para responder algunas preguntas abiertas sobre la relación y el desempeño entre los paradigmas de aprendizaje, los grados de desequilibrio y las soluciones propuestas. Se pueden encontrar estudios analíticos más complejos para analizar las características intrínsecas de los datos (L´opez et al., 2013), factores de dificultad de los datos tales como subconceptos raros de instancias minoritarias, superposición de clases (Luengo, Fern´andez, Garc´ıa, & Herrera, 2011; Stefanowski, 2016) y diferentes tipos de ejemplos de clases minoritarias (Napierala & Stefanowski, 2016).

4. Variaciones de SMOTE a otros paradigmas de aprendizaje 

En esta sección, presentaremos los enfoques basados ​​en SMOTE que abordan otros paradigmas de aprendizaje. En particular, la sección se dividirá en cinco subsecciones, cada una de las cuales proporcionará una descripción general de cada paradigma y las técnicas diseñadas para abordarlo. Se han aplicado extensiones de SMOTE a otros paradigmas de aprendizaje: (1) transmisión de datos (consulte la Sección 4.1); (2) aprendizaje semisupervisado y activo (en la Sección 4.2); (3) Clasificación de instancias múltiples y etiquetas múltiples (Sección 4.3); (4) Regresión (en la Sección 4.4) y (5) Otros problemas de predicción más complejos, como clasificación de texto, clasificación de datos de baja calidad, etc. (ver Sección 4.5). La Tabla 3 presenta un resumen de las extensiones SMOTE por orden cronológico, indicando sus referencias, nombres de algoritmos y paradigmas de aprendizaje que abordan.

A continuación, daremos una breve descripción de cada paradigma de aprendizaje y las técnicas desarrolladas asociadas. 

4.1 Transmisión de datos 

Muchas aplicaciones para algoritmos de aprendizaje necesitan abordar entornos dinámicos donde los datos llegan en forma de transmisión. La naturaleza en línea de los datos crea algunos requisitos computacionales adicionales para un clasificador (Krawczyk et al., 2017). Además, normalmente se requiere que los modelos de predicción se adapten a las derivas del concepto, que son fenómenos derivados de las características no estacionarias de los flujos de datos. En la versión fuera de línea de la clasificación de desequilibrio, el clasificador puede estimar la relación entre la clase minoritaria y la clase mayoritaria antes de que comience el aprendizaje. Sin embargo, en el aprendizaje en línea, esto no es posible debido a que las clases pueden cambiar su distribución a lo largo del tiempo, por lo que deben lidiar con la dinámica de los datos. Se han propuesto dos técnicas de preprocesamiento (Ram´ırez-Gallego et al., 2017) basadas en SMOTE para tratar flujos de datos desequilibrados. El primero es Learn++.NSE-SMOTE (Ditzler & Polikar, 2013), que es una extensión de Learn++.SMOTE (Ditzler et al., 2010). Primero, los autores incorporaron SMOTE dentro del algoritmo Learn++.NSE y luego decidieron reemplazar SMOTE con un subconjunto que hace un uso estratégico de los datos de las clases minoritarias. La segunda técnica es GOS-IL (Barua et al., 2015). Funciona actualizando un alumno base de forma incremental mediante el sobremuestreo estándar. Cuando se recibe un flujo de datos a lo largo del tiempo y disponemos de información temporal, nos referimos a la clasificación de series temporales. Una muestra de datos de series temporales es un conjunto ordenado de variables de valor real que provienen de una señal continua, que puede estar en el dominio temporal o espacial. Las variables cercanas entre sí a menudo están altamente correlacionadas en series de tiempo. Los métodos SPO (Cao et al., 2011) e INOS (Cao et al., 2013) proponen una integración de SMOTE en la clasificación de series temporales. INOS puede verse como una extensión de SPO y aborda el problema del aprendizaje desequilibrado mediante el sobremuestreo de la clase minoritaria en el espacio de la señal. Se utilizó una técnica híbrida para generar ejemplos sintéticos mediante la estimación y el mantenimiento de la estructura de covarianza principal en el subespacio propio fiable y la fijación del espectro propio no fiable. También se ideó una tercera familia de técnicas denominada SM B, SM T y SM TPhi (Moniz et al., 2016) para series temporales, pero para regresión. Los detalles para ellos se darán en la Sección 4.4.

