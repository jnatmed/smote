# Variantes de Smote: una implementación de Python de 85 técnicas de sobremuestreo minoritarias

Los problemas de clasificación desequilibrada definitivamente rondan a He y Gracia (2009), y un enfoque exitoso para evitar el sobreajuste de las clases mayoritarias es la generación sintética de muestras de entrenamiento minoritarias Fernández et al. (2018). A pesar de la gran cantidad de algoritmos de sobremuestreo minoritarios propuestos, las implementaciones de código abierto están disponibles solo para un puñado de técnicas. El paquete smote-variants proporciona una implementación de Python de 85 técnicas de sobremuestreo para impulsar las aplicaciones y el desarrollo en el campo del aprendizaje desequilibrado. El código fuente, la documentación y los ejemplos están disponibles en el repositorio de GitHub http://github.com/gykovacs/smote_variants/.

1. Introducción 

Los problemas de clasificación desequilibrada ganaron mayor interés en los últimos años [1,2]. La literatura [1] distingue tres enfoques principales para manejar el desequilibrio en los problemas de clasificación: entrenamiento sensible a los costos, soluciones específicas del clasificador y sobremuestreo de la(s) clase(s) minoritaria(s). Una de las primeras técnicas de sobremuestreo fue la técnica de sobremuestreo de minorías sintéticas (SMOTE) [3], y una revisión reciente [2] de sus numerosas variantes y aplicaciones muestra claramente la popularidad y el interés en las técnicas de sobremuestreo. A pesar del éxito del sobremuestreo, solo un puñado de técnicas están disponibles en el software de código abierto: el paquete de Python imblearn [4] implementa 7 técnicas de sobremuestreo; el paquete R smotefamily ofrece 4 métodos de sobremuestreo más, y un número limitado de técnicas, como CCR [5], tienen implementaciones públicas e independientes. Dado que hasta ahora se han propuesto más de 85 variantes de SMOTE [2] (de las cuales 9 métodos en los últimos dos años), la falta de implementaciones disponibles claramente retrasa el desarrollo en el campo, lo que dificulta la evaluación adecuada de nuevos métodos. técnicas o seleccione la técnica de mejor rendimiento para un conjunto de datos en particular. El objetivo del paquete Python smote-variants es impulsar la investigación y las aplicaciones en el campo mediante la implementación de 85 técnicas de sobremuestreo en un marco integral. Según nuestro mejor conocimiento, esta es la primera implementación pública de código abierto para 76 sobremuestreadores. Como las versiones actuales de imblearn y smotefamily implementan 9 técnicas de sobremuestreo juntas, no las tratamos como competidores del paquete smote-variants propuesto en alcance o volumen. Además del sobremuestreo binario, smote-variants ofrece multiclase compatible con 61 de los sobremuestreadores binarios implementados y un marco de selección de modelos para encontrar fácilmente el sobremuestreador apropiado para un conjunto de datos en particular. La lista de todas las técnicas implementadas, citas, documentaciones, galerías, códigos de muestra (Python, R y Julia) y una clasificación de los mejores sobremuestreadores en función de su desempeño en 104 conjuntos de datos desequilibrados están disponibles en el repositorio de GitHub http://github.com/gykovacs/smote_variants y en la página de documentación http://smote-variants.readthedocs.io. Las principales contribuciones del paquete al campo: (a) los teóricos pueden comparar fácilmente las técnicas existentes y descubrir los principios operativos más exitosos para impulsar más investigaciones en el campo; (b) los profesionales que trabajan con datos desequilibrados pueden seleccionar los sobremuestreadores más adecuados para mejorar el rendimiento de la clasificación en aplicaciones reales. El documento está organizado de la siguiente manera. En la Sección 2 se da una introducción más detallada al sobremuestreo; en la Sección 3 se discuten la organización del paquete y los detalles de implementación; los resultados empíricos y las ilustraciones se presentan en la Sección 4; y las conclusiones se extraen en la Sección 5.

2. Problemas y antecedentes 

Un problema de clasificación binaria desequilibrada tiene un conjunto de entrenamiento x i ∈ R d , y i ∈ {−, + } , i = 1 , . . . , (N + + N −) que consta de $N_+$ muestras minoritarias y N −mayoritarias con N + << N −. Las técnicas de clasificación convencionales sobreajustan naturalmente a la clase mayoritaria ya que las muestras mayoritarias están demasiado representadas en la función de pérdida. Dado que las técnicas de regularización convencionales están preparadas para equilibrar el sesgo y la varianza, no logran regularizar las degeneraciones unilaterales como el sobreajuste de una clase en detrimento de la otra. Una forma posible de mejorar el rendimiento de la clasificación es hacer suposiciones sobre la distribución espacial local de la clase minoritaria y generar más muestras de entrenamiento minoritario para equilibrar el conjunto de datos. Por ejemplo, la suposición hecha por SMOTE es que los segmentos de línea entre muestras minoritarias vecinas pertenecen a la clase minoritaria. En consecuencia, SMOTE genera instancias minoritarias al muestrear aleatoriamente estos segmentos de línea: sean x 1 y x 2 muestras minoritarias vecinas, se genera una nueva instancia minoritaria como x new = x 1 + r ·(x 2 −x 1 ), donde r ∈ [0, 1] es un número aleatorio uniformemente distribuido. En la última década, se han propuesto numerosas técnicas de sobremuestreo que hacen varias suposiciones sobre la distribución local de los datos y utilizan varios conceptos matemáticos, desde los diagramas de Voronoi hasta la teoría de juegos (para una descripción detallada, consulte [2]), esta variedad de técnicas es ofrecido por el paquete de variantes smote propuesto.

3. Estructura del software y detalles de implementación 

El software está diseñado como un paquete independiente de Python 3.5+, construido principalmente sobre las funcionalidades de aprendizaje automático de sklearn [6] . Las técnicas de sobremuestreo se implementan como clases separadas que proporcionan la función de muestra como una interfaz común, llevando a cabo el sobremuestreo de los conjuntos de datos. Como paquete público, la calidad del código está garantizada por la configuración del sistema de integración continua TravisCI y las pruebas unitarias que cubren más del 95% del código. El paquete ofrece dos funcionalidades principales: (a) sobremuestreo binario y multiclase: cada uno de los sobremuestreadores implementados se puede usar con conjuntos de datos binarios, y 61 de ellos son compatibles con el enfoque de sobremuestreo multiclase implementado; (b) selección de modelos: el paquete proporciona funciones de alto nivel para facilitar la evaluación, la comparación y la selección del sobremuestreador. En la implementación, usamos los mismos acrónimos que [2] y tratamos de ceñirnos a los nombres de variables y estructuras de algoritmos presentados en los documentos originales. Como muchas de las técnicas de sobremuestreo no se especificaron a nivel de pseudocódigo, los detalles algorítmicos menores se completaron con los pasos más significativos para hacer que los algoritmos fueran resistentes a configuraciones inesperadas de datos, todos estos cambios están documentados en las fuentes. Finalmente, mencionamos que el paquete admite la ejecución distribuida en términos del paquete joblib. 

4. Ejemplos ilustrativos 

Para ilustrar el uso del paquete, hemos seleccionado el conjunto de datos libras_move del paquete iblearn y el sobremuestreador OUPS [2]. Mediante el siguiente segmento de código, ilustramos la facilidad del sobremuestreo utilizando smote-variants, los resultados se representan en la Fig. 1: Listado 1: Examples/007_paper_examples.py:96-101.

La puntuación AUC (Área bajo la curva característica operativa del receptor) es una medida de rendimiento comúnmente aceptada en el aprendizaje desequilibrado [5], la puntuación resultante de 0,9789 en comparación con la puntuación de 0,9628 sin sobremuestreo ilustra los beneficios del sobremuestreo en el aprendizaje desequilibrado. 

5. Conclusiones 

El paquete smote-variants proporciona implementación en Python para 85 técnicas de sobremuestreo binario, un enfoque de sobremuestreo multiclase compatible con 61 de los sobremuestreadores binarios implementados, y ofrece varias funcionalidades de validación cruzada y evaluación para facilitar la uso del paquete. Según nuestro mejor conocimiento, para 76 técnicas de sobremuestreo, esta es la primera implementación de código abierto. La lista citada de algoritmos implementados, el código fuente documentado, las galerías y los ejemplos están disponibles en el repositorio de GitHub http://github.com/gykovacs/smote_variants.

6. Declaración de conflicto de intereses 

Deseamos confirmar que no existen conflictos de intereses conocidos asociados con esta publicación y que no ha habido apoyo financiero significativo para este trabajo que podría haber influido en su resultado.