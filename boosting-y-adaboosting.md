### 8.6.3 Impulso y AdaBoost (Adaptative Boosting)
Ahora veremos el método de conjunto de impulso. Al igual que en el apartado anterior, supongamos que como paciente usted tiene ciertos síntomas. En lugar de consultar a un médico, elige consultar a varios. Suponga que asigna ponderaciones al valor del diagnóstico de cada médico, en función de la precisión de los diagnósticos anteriores que hayan realizado. El diagnóstico final es entonces una combinación de los diagnósticos ponderados. Esta es la esencia detrás de impulsar. Al impulsar, también se asignan pesos a cada tupla de entrenamiento. Se aprende iterativamente una serie de k clasificadores. Después de aprender un clasificador, Mi , los pesos se actualizan para permitir que el clasificador posterior, Mi+1, "preste más atención" a las tuplas de entrenamiento que Mi clasificó incorrectamente. El clasificador potenciado final, M∗, combina los votos de cada clasificador individual, donde el peso del voto de cada clasificador es una función de su precisión. AdaBoost (abreviatura de Adaptive Boosting) es un algoritmo de impulso popular. Supongamos que queremos aumentar la precisión de un método de aprendizaje. Tenemos D, un conjunto de datos de d tuplas etiquetadas con clase, (X1, y1),(X2, y2),...,(Xd, yd), donde yi es la etiqueta de clase de la tupla Xi. Inicialmente, AdaBoost asigna a cada tupla de entrenamiento un peso igual de 1/d. La generación de k clasificadores para el conjunto requiere k rondas por el resto del algoritmo. En la ronda i, las tuplas de D se muestrean para formar un conjunto de entrenamiento, Di, de tamaño d. Muestreo