### 8.6.4 Bosques aleatorios 
Ahora presentamos otro método de conjunto llamado bosques aleatorios. Imagine que cada uno de los clasificadores del conjunto es un clasificador de árbol de decisión, de modo que la colección de clasificadores es un "bosque". Los árboles de decisión individuales se generan utilizando una selección aleatoria de atributos en cada nodo para determinar la división. Más formalmente, cada árbol depende de los valores de un vector aleatorio muestreado de forma independiente y con la misma distribución para todos los árboles del bosque. Durante la clasificación, cada árbol vota y se devuelve la clase más popular. Los bosques aleatorios se pueden construir utilizando embolsado (Sección 8.6.2) junto con la selección de atributos aleatorios. Se da un conjunto de entrenamiento, D, de d tuplas. El procedimiento general para generar k árboles de decisión para el conjunto es el siguiente. Para cada iteración, i(i = 1, 2,..., k), se muestrea un conjunto de entrenamiento, Di, de d tuplas con reemplazo de D. Es decir, cada Di es una muestra de arranque de D (Sección 8.5.1). 4), por lo que algunas tuplas pueden aparecer más de una vez en Di, mientras que otras pueden quedar excluidas. Sea F el número de atributos que se utilizarán para determinar la división en cada nodo, donde F es mucho menor que el número de atributos disponibles. Para construir un clasificador de árbol de decisión, Mi , seleccione aleatoriamente, en cada nodo, atributos F como candidatos para la división en el nodo. La metodología CART se utiliza para hacer crecer los árboles. Los árboles crecen hasta su tamaño máximo y no se podan. Los bosques aleatorios formados de esta manera, con selección de entrada aleatoria, se denominan Forest-RI. Otra forma de bosque aleatorio, llamada Forest-RC, utiliza combinaciones lineales aleatorias de los atributos de entrada. En lugar de seleccionar aleatoriamente un subconjunto de atributos, crea nuevos atributos (o funciones) que son una combinación lineal de los atributos existentes. Es decir, se genera un atributo especificando L, el número de atributos originales que se combinarán. En un nodo determinado, L atributos se seleccionan aleatoriamente y se suman junto con coeficientes que son números aleatorios uniformes en [−1,1]. Se generan F combinaciones lineales, y sobre ellas se busca la mejor división. Esta forma de bosque aleatorio es útil cuando solo hay unos pocos atributos disponibles, para reducir la correlación entre clasificadores individuales. Los bosques aleatorios son comparables en precisión a AdaBoost, pero son más resistentes a errores y valores atípicos. El error de generalización para un bosque converge siempre que la cantidad de árboles en el bosque sea grande. Por lo tanto, el sobreajuste no es un problema. La precisión de un bosque aleatorio depende de la fuerza de los clasificadores individuales y de una medida de la dependencia entre ellos. Lo ideal es mantener la fuerza de los clasificadores individuales sin aumentar su correlación. Los bosques aleatorios son insensibles al número de atributos seleccionados para su consideración en cada división. Por lo general, se eligen hasta $log_{2}(d + 1)$. (Una observación empírica interesante fue que el uso de un solo atributo de entrada aleatorio puede dar como resultado una buena precisión que a menudo es mayor que cuando se usan varios atributos). Debido a que los bosques aleatorios consideran muchos menos atributos para cada división, son eficientes en bases de datos muy grandes. Pueden ser más rápidos que embolsar o impulsar. Los bosques aleatorios dan estimaciones internas de importancia variable.