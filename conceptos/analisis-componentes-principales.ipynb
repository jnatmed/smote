{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de Componentes principales (PCA)\n",
    "El análisis de componentes principales (PCA) es una técnica multivariada que analiza una tabla de datos en la que las observaciones se describen mediante varias variables dependientes cuantitativas interrelacionadas. Su objetivo es extraer la información importante de la tabla, representarla como un conjunto de nuevas variables ortogonales llamadas componentes principales y mostrar el patrón de similitud de las observaciones y de las variables como puntos en mapas. La calidad del modelo PCA se puede evaluar utilizando técnicas de validación cruzada como bootstrap y jackknife. PCA se puede generalizar como análisis de correspondencia (CA) para manejar variables cualitativas y como análisis de factores múltiples (MFA) para manejar conjuntos heterogéneos de variables. Matemáticamente, PCA depende de la descomposición propia de matrices semidefinidas positivas y de la descomposición en valores singulares (SVD) de matrices rectangulares.  2010 John Wiley & Sons, Inc. WIREs Comp Stat 2010 2 433–459.\n",
    "\n",
    "El análisis de componentes principales (PCA) es probablemente la técnica estadística multivariante más popular y es utilizada por casi todas las disciplinas científicas. También es probable que sea la técnica multivariante más antigua. De hecho, su origen se remonta a Pearson1 o incluso Cauchy2 [ver Ref 3, p. 416], o Jordan4 y también Cayley, Silverster y Hamilton, [ver Refs 5,6, para más detalles] pero su ejemplificación moderna fue formalizada por Hotelling7 quien también acuñó el término componente principal. PCA analiza una tabla de datos que representa las observaciones descritas por varias variables dependientes que, en general, están interrelacionadas. Su objetivo es extraer la información importante de la tabla de datos y expresar esta información como un conjunto de nuevas variables ortogonales llamadas componentes principales. PCA también representa el patrón de similitud de las observaciones y las variables mostrándolas como puntos en mapas [ver Refs 8–10 para más detalles].\n",
    "\n",
    "OBJETIVOS DE PCA \n",
    "Los objetivos de PCA son:\n",
    " (1) extraer la información más importante de la tabla de datos; \n",
    " (2) comprimir el tamaño del conjunto de datos manteniendo solo esta información importante; \n",
    " (3) simplificar la descripción del conjunto de datos; y \n",
    " (4) analizar la estructura de las observaciones y las variables. Para lograr estos objetivos, PCA calcula nuevas variables llamadas componentes principales que se obtienen como combinaciones lineales de las variables originales. Se requiere que el primer componente principal tenga la mayor varianza posible (es decir, inercia y, por lo tanto, este componente \"explicará\" o \"extraerá\" la mayor parte de la inercia de la tabla de datos). El segundo componente se calcula bajo la restricción de ser ortogonal al primer componente y tener la mayor inercia posible. \n",
    " Los otros componentes se calculan de la misma manera (consulte el Apéndice A para ver la prueba). Los valores de estas nuevas variables para las observaciones se denominan puntajes de factores, y estos puntajes de factores pueden interpretarse geométricamente como las proyecciones de las observaciones sobre los componentes principales.\n",
    "\n",
    "Hallar los componentes \n",
    "En PCA, los componentes se obtienen del SVD de la tabla de datos X. Específicamente, con $X = P\\triangle Q^T$ (cf. Ec. 1), la matriz $I × L$ de puntajes factoriales, denotada por F, se obtiene como :\n",
    "\n",
    "$$F = P \\triangle $$\n",
    "\n",
    "FIGURA 1 | Los pasos geométricos para encontrar los componentes de un análisis de componentes principales. Para encontrar los componentes (1), centre las variables y luego grafica una contra la otra. (2) Encuentre la dirección principal (llamada la primera componente) de la nube de puntos tal que tengamos el mínimo de la suma de las distancias al cuadrado de los puntos a la componente. Agregue un segundo componente ortogonal al primero de modo que la suma de las distancias al cuadrado sea mínima. (3) Cuando se hayan encontrado los componentes, gire la figura para colocar el primer componente horizontalmente (y el segundo componente verticalmente), luego borre los ejes originales. Tenga en cuenta que el gráfico final podría haberse obtenido directamente trazando las observaciones de las coordenadas dadas en la Tabla 1.\n",
    "\n",
    "TABLA 1 Puntuaciones brutas, desviaciones de la media, coordenadas, coordenadas al cuadrado de los componentes, contribuciones de las observaciones a los componentes, distancias al centro de gravedad al cuadrado y cosenos al cuadrado de las observaciones para el ejemplo de longitud de palabras (Y) y Número de líneas (W)\n",
    "\n",
    "M W = 8, MY = 6. Las siguientes abreviaturas se utilizan para etiquetar las columnas: w = (W − M W); y = (Y − MI). Las contribuciones y los cosenos cuadrados se multiplican por 100 para facilitar la lectura. Las contribuciones importantes positivas están en cursiva y las contribuciones importantes negativas están representadas en negrita.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Faltantes\n",
    "\n",
    "La minería de datos (DM) es la extracción de información predictiva oculta de grandes bases de datos (DB). Con el descubrimiento automático de conocimiento implícito dentro de las bases de datos, DM utiliza sofisticadas técnicas de modelado y análisis estadístico para descubrir patrones y relaciones ocultas en las bases de datos organizacionales. Durante los últimos 40 años, las herramientas y técnicas para procesar información estructurada han seguido evolucionando desde bases de datos hasta almacenamiento de datos (DW) y DM. Las aplicaciones DW se han vuelto críticas para el negocio. DM puede extraer aún más valor de estos enormes depósitos de información. Los enfoques de la DM son variados y, a menudo, confusos. Este libro presenta una visión general del estado del arte en este campo nuevo y multidisciplinario. DM está despegando por varias razones: las organizaciones están recopilando más datos sobre sus negocios, los costos de almacenamiento se han reducido drásticamente y las presiones comerciales competitivas han aumentado. Otros factores incluyen la aparición de presiones para controlar las inversiones en TI existentes y, por último, pero no menos importante, la marcada reducción en la relación costo/rendimiento de los sistemas informáticos. Existen cuatro operaciones mineras básicas respaldadas por numerosas técnicas mineras: creación de modelos predictivos respaldada por técnicas de inducción supervisada; análisis de enlaces apoyado por técnicas de descubrimiento de asociaciones y descubrimiento de secuencias; Segmentación de bases de datos respaldada por técnicas de agrupamiento; y detección de desviaciones apoyada en técnicas estadísticas. Aunque DM todavía está en pañales, las empresas de una amplia gama de industrias, incluidas la venta minorista, la banca y las finanzas, la atención de la salud, la fabricación, las telecomunicaciones y la industria aeroespacial, así como las agencias gubernamentales, ya están utilizando herramientas y técnicas de DM para aprovechar las ventajas históricas. datos. Mediante el uso de tecnologías de reconocimiento de patrones y técnicas estadísticas y matemáticas para filtrar la información almacenada, DM ayuda a los analistas a reconocer hechos, relaciones, tendencias, patrones, excepciones y anomalías significativos que, de otro modo, podrían pasar desapercibidos. En mi convocatoria de capítulos de febrero de 2001, busqué contribuciones para este libro que abordaran una gran cantidad de temas que van desde el avance de nuevas teorías hasta estudios de casos de experiencias de empresas con su DM. Después de pasar un año y medio de preparación en el libro y un estricto proceso arbitrado, estoy encantado de verlo aparecer en el mercado. El objetivo principal de este libro es explorar la miríada de temas relacionados con la DM, centrándose específicamente en aquellas áreas que exploran nuevas metodologías o examinan estudios de casos. Los lectores previstos de este libro son un amplio espectro de científicos, profesionales, estudiantes de posgrado y gerentes que realizan investigaciones y/o implementan los descubrimientos. El libro contiene una colección de veinte capítulos escritos por un equipo verdaderamente internacional de cuarenta y cuatro expertos que representan a los principales científicos y jóvenes académicos talentosos de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores perdidos en la matriz de datos \n",
    "Los datos recopilados de los experimentos rara vez son perfectos. El problema de los datos faltantes y erróneos es un campo muy amplio en la literatura estadística. En primer lugar, existe la posibilidad de que la \"falta\" de valores de datos sea significativa para el análisis, en cuyo caso la falta debe modelarse como un valor de datos ordinario. Luego, el problema se ha internalizado y el análisis puede continuar como de costumbre, con la diferencia importante de que los valores faltantes no están disponibles para el análisis. Ramoni y Sebastiani (1998) desarrollaron un enfoque más escéptico, quienes consideraron una opción para considerar los valores faltantes como adversarios (las conclusiones sobre la estructura de dependencia serían entonces verdaderas, sin importar cuáles sean los valores faltantes, pero con muchos datos faltantes). , las conclusiones se volverán muy débiles). Una tercera posibilidad es que se sepa que la ausencia no tiene nada que ver con los objetivos del análisis: faltan datos completamente al azar. Por ejemplo, en una aplicación médica, si faltan datos debido al mal estado del paciente, la falta de datos es importante si la investigación se refiere a pacientes. Pero si faltan datos debido a la falta de disponibilidad del equipo, probablemente no sea así, a menos que tal vez la investigación esté relacionada con la calidad del hospital. Suponiendo que los datos faltan completamente al azar, es relativamente fácil obtener un análisis adecuado. No es necesario desperdiciar cajas enteras solo porque falta un elemento. La mayoría de los análisis realizados se refieren solo a un pequeño número de columnas, y estas columnas se pueden comparar para todos los casos en los que no faltan datos en estas columnas en particular. De esta forma, por ejemplo, es posible hacer un modelo gráfico para un conjunto de datos incluso si a cada caso le falta algún elemento, ya que todos los cálculos de la sección de elección del modelo gráfico se refieren a un pequeño número de columnas. En esta situación, incluso es posible imputar los valores faltantes, porque el modelo gráfico obtenido muestra qué variables influyen más en el faltante. Entonces, cada valor que falta para una variable se puede predecir a partir de los valores del caso para los vecinos de la variable en el gráfico del modelo. Cuando se hace esto, siempre se debe recordar que el valor es una suposición. Por lo tanto, nunca se puede usar para crear una medida de significado formal; eso sería equivalente a usar los mismos datos dos veces, lo que no está permitido en la inferencia formal. En el Capítulo 7 se puede encontrar una revisión completa del problema de los datos faltantes. El método de imputación de valores faltantes tiene una buena formalización en el método de maximización de expectativas (EM). modelo de los datos. En el primer paso, lo que no falta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
