## [XGBoost: un sistema escalable de refuerzo de árboles](https://arxiv.org/pdf/1603.02754.pdf)

RESUMEN El impulso de árboles es un método de aprendizaje automático altamente efectivo y ampliamente utilizado. En este documento, describimos un sistema escalable de refuerzo de árbol de extremo a extremo llamado XGBoost, que los científicos de datos utilizan ampliamente para lograr resultados de vanguardia en muchos desafíos de aprendizaje automático. Proponemos un algoritmo novedoso consciente de la escasez para datos escasos y un bosquejo de cuantiles ponderados para el aprendizaje aproximado del árbol. Más importante aún, brindamos información sobre los patrones de acceso a la memoria caché, la compresión de datos y la fragmentación para crear un sistema escalable de refuerzo de árboles. Al combinar estos conocimientos, XGBoost escala más allá de miles de millones de ejemplos utilizando muchos menos recursos que los sistemas existentes.

1. INTRODUCCIÓN El aprendizaje automático y los enfoques basados ​​en datos se están volviendo muy importantes en muchas áreas. Los clasificadores de spam inteligentes protegen nuestro correo electrónico aprendiendo de cantidades masivas de datos de spam y comentarios de los usuarios; los sistemas publicitarios aprenden a relacionar los anuncios correctos con el contexto correcto; los sistemas de detección de fraude protegen a los bancos de atacantes malintencionados; Los sistemas de detección de eventos anómalos ayudan a los físicos experimentales a encontrar eventos que conduzcan a una nueva física. Hay dos factores importantes que impulsan estas aplicaciones exitosas: el uso de modelos efectivos (estadísticos) que capturan las dependencias de datos complejas y los sistemas de aprendizaje escalables que aprenden el modelo de interés de grandes conjuntos de datos. Entre los métodos de aprendizaje automático que se utilizan en la práctica, el refuerzo del árbol de gradiente [10] 1 es una técnica que destaca en muchas aplicaciones. Se ha demostrado que el impulso de árboles brinda resultados de vanguardia en muchos puntos de referencia de clasificación estándar [16]. LambdaMART [5], una variante de impulso de árbol para clasificación, logra resultados de última generación para problemas de clasificación. Además de ser utilizado como un predictor independiente, también se incorpora a las canalizaciones de producción del mundo real para la predicción de la tasa de clics de anuncios [15]. Finalmente, es la elección de facto del método de conjunto y se utiliza en desafíos como el premio Netflix [3]. En este documento, describimos XGBoost, un sistema escalable de aprendizaje automático para potenciar árboles. El sistema está disponible como un paquete de código abierto2. El impacto del sistema ha sido ampliamente reconocido en una serie de desafíos de aprendizaje automático y minería de datos. Tome los desafíos organizados por el sitio de competencia de aprendizaje automático Kaggle, por ejemplo. Entre las 29 soluciones ganadoras del desafío 3 publicadas en el blog de Kaggle durante 2015, 17 soluciones utilizaron XGBoost. Entre estas soluciones, ocho solo usaron XGBoost para entrenar el modelo, mientras que la mayoría de las demás combinaron XGBoost con redes neuronales en conjuntos. A modo de comparación, el segundo método más popular, redes neuronales profundas, se utilizó en 11 soluciones. El éxito del sistema también se vio en la KDDCup 2015, donde todos los equipos ganadores del top 10 utilizaron XGBoost. Además, los equipos ganadores informaron que los métodos de conjunto superan a un XGBoost bien configurado por solo una pequeña cantidad [1]. Estos resultados demuestran que nuestro sistema ofrece resultados de última generación en una amplia gama de problemas. Los ejemplos de los problemas en estas soluciones ganadoras incluyen: predicción de ventas en tiendas; clasificación de eventos de física de alta energía; clasificación de textos web; predicción del comportamiento del cliente; detección de movimiento; predicción de la tasa de clics en los anuncios; clasificación de malware; categorización de productos; predicción del riesgo de peligros; predicción masiva de la tasa de abandono de cursos en línea. Si bien el análisis de datos dependientes del dominio y la ingeniería de características juegan un papel importante en estas soluciones, el hecho de que XGBoost sea la elección consensuada de los estudiantes muestra el impacto y la importancia de nuestro sistema y el refuerzo de árboles. 

El factor más importante detrás del éxito de XGBoost es su escalabilidad en todos los escenarios. El sistema se ejecuta más de diez veces más rápido que las soluciones populares existentes en una sola máquina y escala a miles de millones de ejemplos en configuraciones distribuidas o con memoria limitada. La escalabilidad de XGBoost se debe a varios sistemas importantes y optimizaciones algorítmicas. Estas innovaciones incluyen: 
* un novedoso algoritmo de aprendizaje de árboles para manejar datos dispersos; 
* un procedimiento de bosquejo de cuantiles ponderado teóricamente justificado permite manejar pesos de instancia en el aprendizaje de árboles aproximados. La computación paralela y distribuida hace que el aprendizaje sea más rápido, lo que permite una exploración más rápida del modelo. Más importante aún, XGBoost aprovecha la computación fuera del núcleo y permite a los científicos de datos procesar cientos de millones de ejemplos en un escritorio. Por último, es aún más interesante combinar estas técnicas para crear un sistema integral que se adapte a datos aún más grandes con la menor cantidad de recursos de clúster. Las principales contribuciones de este documento se enumeran a continuación: 
• Diseñamos y construimos un sistema de refuerzo de árboles de extremo a extremo altamente escalable. 
• Proponemos un esquema de cuantiles ponderados teóricamente justificado para el cálculo eficiente de la propuesta. 
• Presentamos un novedoso algoritmo consciente de la escasez para el aprendizaje de árboles paralelos. 
• Proponemos una estructura de bloque con reconocimiento de memoria caché eficaz para el aprendizaje de árboles fuera del núcleo. Si bien existen algunos trabajos existentes sobre el refuerzo de árboles paralelos [22, 23, 19], no se han explorado las direcciones como el cálculo fuera del núcleo, el aprendizaje consciente de la memoria caché y consciente de la escasez. Más importante aún, un sistema de extremo a extremo que combina todos estos aspectos brinda una solución novedosa para casos de uso del mundo real. Esto permite a los científicos de datos y a los investigadores crear poderosas variantes de algoritmos de refuerzo de árboles [7, 8]. Además de estas importantes contribuciones, también realizamos mejoras adicionales al proponer un objetivo de aprendizaje regularizado, que incluiremos para completarlo. El resto del documento está organizado de la siguiente manera. 
- Primero revisaremos el impulso del árbol e introduciremos un objetivo regularizado en la Sec. 2. 
- Luego describimos los métodos de búsqueda dividida en la Sec. 3, así como el diseño del sistema en la Sec. 4, incluidos los resultados experimentales cuando sea relevante para proporcionar apoyo cuantitativo para cada optimización que describimos. El trabajo relacionado se discute en la Sec. 5. Las evaluaciones detalladas de extremo a extremo se incluyen en la Sec. 6. Finalmente concluimos el artículo en la Sec. 7.

2. IMPULSO DE ÁRBOLES EN POCAS PALABRAS 
Revisamos los algoritmos de impulso de árboles de gradiente en esta sección. La derivación se deriva de la misma idea en la literatura existente sobre aumento de gradiente. Específicamente, el método de segundo orden tiene su origen en Friedman et al. [12]. Realizamos mejoras menores en el objetivo regularizado, que resultaron útiles en la práctica. 2.1 Objetivo de aprendizaje regularizado Para un conjunto de datos dado con n ejemplos y m características D = {(xi, yi)} (|D| = n, xi ∈ R m, yi ∈ R), un modelo de conjunto de árbol (que se muestra en la Fig. 1) usa K funciones aditivas para predecir la salida.

$$ y_i = fi(x_i)=\sum_{k}^{k=1}f_k(x_i, f_k)∈F$$

[What is XGBoost](https://www.kaggle.com/dansbecker/xgboost)
### Qué es XGBoost? 
XGBoost es el modelo líder para trabajar con datos tabulares estándar (el tipo de datos que almacena en Pandas DataFrames, a diferencia de tipos de datos más exóticos como imágenes y videos). Los modelos XGBoost dominan muchas competencias de Kaggle. Para alcanzar la máxima precisión, los modelos XGBoost requieren más conocimiento y ajuste del modelo que técnicas como Random Forest. Después de este tutorial, podrá seguir el flujo de trabajo de modelado completo con XGBoost. Ajustar modelos XGBoost para un rendimiento óptimo. XGBoost es una implementación del algoritmo Gradient Boosted Decision Trees (scikit-learn tiene otra versión de este algoritmo, pero XGBoost tiene algunas ventajas técnicas.) ¿Qué son los árboles de decisión potenciados por gradientes? Recorreremos un diagrama.


